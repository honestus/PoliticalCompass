{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bRQGjNzh9idQ"
   },
   "source": [
    "## Classification with Sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b1J4daS74g5D"
   },
   "source": [
    "## loading a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_col = 'author_flair_text_str'\n",
    "polcompass_df = pd.read_parquet('./data/dataset_whole/polcompass_df.parquet').sort_values(by=['author','created_utc','subreddit'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "      <th>polcompass_flair</th>\n",
       "      <th>horizontal_polcompass_flair</th>\n",
       "      <th>vertical_polcompass_flair</th>\n",
       "      <th>Political</th>\n",
       "      <th>HorizontalDimension</th>\n",
       "      <th>VerticalDimension</th>\n",
       "      <th>Populism</th>\n",
       "      <th>PeopleCentrism</th>\n",
       "      <th>AntiElitism</th>\n",
       "      <th>EmotionalAppeal</th>\n",
       "      <th>Libertarian</th>\n",
       "      <th>Authoritarian</th>\n",
       "      <th>Left</th>\n",
       "      <th>Right</th>\n",
       "      <th>Motivazioni</th>\n",
       "      <th>full_response</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>content_id</th>\n",
       "      <th>content_type</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>197648.0</th>\n",
       "      <th>post</th>\n",
       "      <td>How Solar Power Caused the Ukraine War</td>\n",
       "      <td>2455</td>\n",
       "      <td>Undefined</td>\n",
       "      <td>Undefined</td>\n",
       "      <td>Undefined</td>\n",
       "      <td>True</td>\n",
       "      <td>Undefined</td>\n",
       "      <td>Undefined</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>Il titolo è provocatorio e suggerisce un colle...</td>\n",
       "      <td>Ok, ecco l'analisi del testo fornito, nel form...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197749.0</th>\n",
       "      <th>post</th>\n",
       "      <td>Glenn Greenwald - The DOJ completely failed to...</td>\n",
       "      <td>2455</td>\n",
       "      <td>libleft</td>\n",
       "      <td>left</td>\n",
       "      <td>lib</td>\n",
       "      <td>True</td>\n",
       "      <td>left</td>\n",
       "      <td>lib</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.10</td>\n",
       "      <td>The text criticizes the DOJ and FBI, highlight...</td>\n",
       "      <td>score__Political__score:value__Political__valu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>565947.0</th>\n",
       "      <th>post</th>\n",
       "      <td>How texaco helped franco win the spanish civil...</td>\n",
       "      <td>6431</td>\n",
       "      <td>authleft</td>\n",
       "      <td>left</td>\n",
       "      <td>auth</td>\n",
       "      <td>True</td>\n",
       "      <td>left</td>\n",
       "      <td>None</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.05</td>\n",
       "      <td>Il testo è chiaramente politico, riferendosi a...</td>\n",
       "      <td>Ok, ecco l'output basato sul testo fornito e l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>648512.0</th>\n",
       "      <th>post</th>\n",
       "      <td>Viva la révolution!</td>\n",
       "      <td>7273</td>\n",
       "      <td>libleft</td>\n",
       "      <td>left</td>\n",
       "      <td>lib</td>\n",
       "      <td>True</td>\n",
       "      <td>left</td>\n",
       "      <td>lib</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.10</td>\n",
       "      <td>The phrase \"Viva la révolution!\" is inherently...</td>\n",
       "      <td>Ok, ecco l'analisi del testo fornito, nel form...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>696748.0</th>\n",
       "      <th>post</th>\n",
       "      <td>Concern about JPs embrace of a neocon like Dan...</td>\n",
       "      <td>7677</td>\n",
       "      <td>libright</td>\n",
       "      <td>right</td>\n",
       "      <td>lib</td>\n",
       "      <td>True</td>\n",
       "      <td>right</td>\n",
       "      <td>lib</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.70</td>\n",
       "      <td>The text expresses concern about Jordan Peters...</td>\n",
       "      <td>```json\\n{\\n    \"score__Political__score\": \"va...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48848923.0</th>\n",
       "      <th>comment</th>\n",
       "      <td>How is she in a cult. Because she doesn’t agre...</td>\n",
       "      <td>35162</td>\n",
       "      <td>centrist</td>\n",
       "      <td>centrist</td>\n",
       "      <td>centrist</td>\n",
       "      <td>True</td>\n",
       "      <td>right</td>\n",
       "      <td>centrist</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.60</td>\n",
       "      <td>Il testo esprime un forte dissenso verso la \"a...</td>\n",
       "      <td>Ok, ecco l'analisi del testo fornito:\\n\\nscore...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49115586.0</th>\n",
       "      <th>comment</th>\n",
       "      <td>Oh boy,\\n\\n&amp;amp;#x200B;\\n\\n&amp;gt;male-woman==mar...</td>\n",
       "      <td>35380</td>\n",
       "      <td>authright</td>\n",
       "      <td>right</td>\n",
       "      <td>auth</td>\n",
       "      <td>True</td>\n",
       "      <td>right</td>\n",
       "      <td>lib</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.40</td>\n",
       "      <td>The text defends a judicial nominee against cl...</td>\n",
       "      <td>```json\\n{\\n  \"score__Political__score\": \"valu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49587578.0</th>\n",
       "      <th>comment</th>\n",
       "      <td>I'm from Europe so nobody I know has reasons t...</td>\n",
       "      <td>35763</td>\n",
       "      <td>libcenter</td>\n",
       "      <td>centrist</td>\n",
       "      <td>lib</td>\n",
       "      <td>True</td>\n",
       "      <td>centrist</td>\n",
       "      <td>centrist</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.30</td>\n",
       "      <td>The text discusses Trump being \"outside of the...</td>\n",
       "      <td>score__Political__score:value__Political__valu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49611632.0</th>\n",
       "      <th>comment</th>\n",
       "      <td>List: GOP Assembly\\n\\nParty: GOP\\n\\nDistrict: ...</td>\n",
       "      <td>35781</td>\n",
       "      <td>libright</td>\n",
       "      <td>right</td>\n",
       "      <td>lib</td>\n",
       "      <td>True</td>\n",
       "      <td>right</td>\n",
       "      <td>auth</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.60</td>\n",
       "      <td>The text is a simple listing of a GOP (Republi...</td>\n",
       "      <td>score__Political__score:value__Political__valu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49707478.0</th>\n",
       "      <th>comment</th>\n",
       "      <td>What argument could I possibly provide to some...</td>\n",
       "      <td>35854</td>\n",
       "      <td>libleft</td>\n",
       "      <td>left</td>\n",
       "      <td>lib</td>\n",
       "      <td>True</td>\n",
       "      <td>left</td>\n",
       "      <td>lib</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.10</td>\n",
       "      <td>Il commento esprime una forte critica verso ch...</td>\n",
       "      <td>Ok, ecco l'output formattato in base alle tue ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>599 rows × 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                      text  \\\n",
       "content_id content_type                                                      \n",
       "197648.0   post                    How Solar Power Caused the Ukraine War    \n",
       "197749.0   post          Glenn Greenwald - The DOJ completely failed to...   \n",
       "565947.0   post          How texaco helped franco win the spanish civil...   \n",
       "648512.0   post                                       Viva la révolution!    \n",
       "696748.0   post          Concern about JPs embrace of a neocon like Dan...   \n",
       "...                                                                    ...   \n",
       "48848923.0 comment       How is she in a cult. Because she doesn’t agre...   \n",
       "49115586.0 comment       Oh boy,\\n\\n&amp;#x200B;\\n\\n&gt;male-woman==mar...   \n",
       "49587578.0 comment       I'm from Europe so nobody I know has reasons t...   \n",
       "49611632.0 comment       List: GOP Assembly\\n\\nParty: GOP\\n\\nDistrict: ...   \n",
       "49707478.0 comment       What argument could I possibly provide to some...   \n",
       "\n",
       "                         author polcompass_flair horizontal_polcompass_flair  \\\n",
       "content_id content_type                                                        \n",
       "197648.0   post            2455        Undefined                   Undefined   \n",
       "197749.0   post            2455          libleft                        left   \n",
       "565947.0   post            6431         authleft                        left   \n",
       "648512.0   post            7273          libleft                        left   \n",
       "696748.0   post            7677         libright                       right   \n",
       "...                         ...              ...                         ...   \n",
       "48848923.0 comment        35162         centrist                    centrist   \n",
       "49115586.0 comment        35380        authright                       right   \n",
       "49587578.0 comment        35763        libcenter                    centrist   \n",
       "49611632.0 comment        35781         libright                       right   \n",
       "49707478.0 comment        35854          libleft                        left   \n",
       "\n",
       "                        vertical_polcompass_flair Political  \\\n",
       "content_id content_type                                       \n",
       "197648.0   post                         Undefined      True   \n",
       "197749.0   post                               lib      True   \n",
       "565947.0   post                              auth      True   \n",
       "648512.0   post                               lib      True   \n",
       "696748.0   post                               lib      True   \n",
       "...                                           ...       ...   \n",
       "48848923.0 comment                       centrist      True   \n",
       "49115586.0 comment                           auth      True   \n",
       "49587578.0 comment                            lib      True   \n",
       "49611632.0 comment                            lib      True   \n",
       "49707478.0 comment                            lib      True   \n",
       "\n",
       "                        HorizontalDimension VerticalDimension  Populism  \\\n",
       "content_id content_type                                                   \n",
       "197648.0   post                   Undefined         Undefined      0.05   \n",
       "197749.0   post                        left               lib      0.60   \n",
       "565947.0   post                        left              None      0.30   \n",
       "648512.0   post                        left               lib      0.60   \n",
       "696748.0   post                       right               lib      0.60   \n",
       "...                                     ...               ...       ...   \n",
       "48848923.0 comment                    right          centrist      0.60   \n",
       "49115586.0 comment                    right               lib      0.10   \n",
       "49587578.0 comment                 centrist          centrist      0.30   \n",
       "49611632.0 comment                    right              auth      0.10   \n",
       "49707478.0 comment                     left               lib      0.60   \n",
       "\n",
       "                         PeopleCentrism  AntiElitism  EmotionalAppeal  \\\n",
       "content_id content_type                                                 \n",
       "197648.0   post                     0.1          0.1              0.4   \n",
       "197749.0   post                     0.7          0.8              0.7   \n",
       "565947.0   post                     0.4          0.5              0.6   \n",
       "648512.0   post                     0.7          0.7              0.8   \n",
       "696748.0   post                     0.5          0.7              0.6   \n",
       "...                                 ...          ...              ...   \n",
       "48848923.0 comment                  0.5          0.7              0.7   \n",
       "49115586.0 comment                  0.3          0.2              0.4   \n",
       "49587578.0 comment                  0.4          0.5              0.3   \n",
       "49611632.0 comment                  0.2          0.1              0.1   \n",
       "49707478.0 comment                  0.7          0.8              0.7   \n",
       "\n",
       "                         Libertarian  Authoritarian  Left  Right  \\\n",
       "content_id content_type                                            \n",
       "197648.0   post                  0.0            0.0   0.0   0.00   \n",
       "197749.0   post                  0.7            0.2   0.7   0.10   \n",
       "565947.0   post                  0.1            0.6   0.5   0.05   \n",
       "648512.0   post                  0.6            0.1   0.6   0.10   \n",
       "696748.0   post                  0.6            0.3   0.2   0.70   \n",
       "...                              ...            ...   ...    ...   \n",
       "48848923.0 comment               0.2            0.1   0.1   0.60   \n",
       "49115586.0 comment               0.6            0.2   0.2   0.40   \n",
       "49587578.0 comment               0.3            0.2   0.3   0.30   \n",
       "49611632.0 comment               0.2            0.4   0.1   0.60   \n",
       "49707478.0 comment               0.6            0.2   0.7   0.10   \n",
       "\n",
       "                                                               Motivazioni  \\\n",
       "content_id content_type                                                      \n",
       "197648.0   post          Il titolo è provocatorio e suggerisce un colle...   \n",
       "197749.0   post          The text criticizes the DOJ and FBI, highlight...   \n",
       "565947.0   post          Il testo è chiaramente politico, riferendosi a...   \n",
       "648512.0   post          The phrase \"Viva la révolution!\" is inherently...   \n",
       "696748.0   post          The text expresses concern about Jordan Peters...   \n",
       "...                                                                    ...   \n",
       "48848923.0 comment       Il testo esprime un forte dissenso verso la \"a...   \n",
       "49115586.0 comment       The text defends a judicial nominee against cl...   \n",
       "49587578.0 comment       The text discusses Trump being \"outside of the...   \n",
       "49611632.0 comment       The text is a simple listing of a GOP (Republi...   \n",
       "49707478.0 comment       Il commento esprime una forte critica verso ch...   \n",
       "\n",
       "                                                             full_response  \n",
       "content_id content_type                                                     \n",
       "197648.0   post          Ok, ecco l'analisi del testo fornito, nel form...  \n",
       "197749.0   post          score__Political__score:value__Political__valu...  \n",
       "565947.0   post          Ok, ecco l'output basato sul testo fornito e l...  \n",
       "648512.0   post          Ok, ecco l'analisi del testo fornito, nel form...  \n",
       "696748.0   post          ```json\\n{\\n    \"score__Political__score\": \"va...  \n",
       "...                                                                    ...  \n",
       "48848923.0 comment       Ok, ecco l'analisi del testo fornito:\\n\\nscore...  \n",
       "49115586.0 comment       ```json\\n{\\n  \"score__Political__score\": \"valu...  \n",
       "49587578.0 comment       score__Political__score:value__Political__valu...  \n",
       "49611632.0 comment       score__Political__score:value__Political__valu...  \n",
       "49707478.0 comment       Ok, ecco l'output formattato in base alle tue ...  \n",
       "\n",
       "[599 rows x 18 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from author_flair_mapping import map_mother_categories\n",
    "df = pd.read_parquet('./data/results_gemini.parquet')\n",
    "category_col = 'polcompass_flair'\n",
    "\n",
    "political_only = True\n",
    "\n",
    "df.Populism = df.Populism.astype(float)\n",
    "df.AntiElitism = df.AntiElitism.astype(float)\n",
    "df.PeopleCentrism = df.PeopleCentrism.astype(float)\n",
    "df.EmotionalAppeal = df.EmotionalAppeal.astype(float)\n",
    "df.Libertarian = df.Libertarian.astype(float)\n",
    "df.Authoritarian = df.Authoritarian.astype(float)\n",
    "df.Left = df.Left.astype(float)\n",
    "df.Right = df.Right.astype(float)\n",
    "\n",
    "mapping_gemini = {'Right': 'right', 'Left': 'left', 'Centrist':'centrist', 'Libertarian':'lib', 'Authoritarian':'auth', 'Centrist':'centrist', 'None':None, 'Political':True, 'NonPolitical':False}\n",
    "def mapping_gemini_cat(curr_cat):\n",
    "    \"\"\"dimension = author_flair_mapping.__map_to_valid_dimension__(dimension)\n",
    "    if dimension is None:\n",
    "        raise ValueError('Dimension is not correct')\n",
    "    curr_mapping = h_mapping_gemini if dimension==author_flair_mapping.HORIZONTAL_DIMENSION else v_mapping_gemini if dimension==author_flair_mapping.VERTICAL_DIMENSION else {}\n",
    "    \"\"\"\n",
    "    return mapping_gemini.get(curr_cat, curr_cat)\n",
    "    \n",
    "    \n",
    "df['Political'] = df.Political.map(mapping_gemini_cat)#.fillna(df['HorizontalDimension'])\n",
    "df['HorizontalDimension'] = df.HorizontalDimension.map(mapping_gemini_cat)#.fillna(df['HorizontalDimension'])\n",
    "df['VerticalDimension'] = df.VerticalDimension.map(mapping_gemini_cat)\n",
    "\n",
    "df = map_mother_categories(df.copy(), category_col).reindex(['text','author', 'polcompass_flair', \n",
    "        'horizontal_polcompass_flair', 'vertical_polcompass_flair', 'Political', 'HorizontalDimension',\n",
    "       'VerticalDimension', 'Populism', 'PeopleCentrism', 'AntiElitism',\n",
    "       'EmotionalAppeal', 'Libertarian', 'Authoritarian', 'Left', 'Right',\n",
    "       'Motivazioni', 'full_response', ], axis=1).sort_index().sort_values(by='author')\n",
    "\n",
    "\n",
    "none_dataset_h = df[(df.HorizontalDimension.fillna('None').isin([None, 'none', 'None'])) & (df[['Left', 'Right']].max(axis=1)<=0.2) ]\n",
    "#none_dataset_h.loc[none_dataset_h.index, [category_col, 'HorizontalDimension', 'horizontal_'+category_col ]] = 'Undefined'\n",
    "none_dataset_v = df[(df.VerticalDimension.fillna('None').isin([None, 'none', 'None'])) & (df[['Libertarian', 'Authoritarian']].max(axis=1)<=0.2) ]\n",
    "#none_dataset_v.loc[none_dataset_v.index, [category_col, 'VerticalDimension', 'vertical_'+category_col, ]] = 'Undefined'\n",
    "none_dataset = pd.concat([none_dataset_h, none_dataset_v],axis=0, ignore_index=False)\n",
    "none_dataset.loc[none_dataset_h.index, [category_col, 'HorizontalDimension', 'horizontal_'+category_col ]] = 'Undefined'\n",
    "none_dataset.loc[none_dataset_v.index, [category_col, 'VerticalDimension', 'vertical_'+category_col, ]] = 'Undefined'\n",
    "none_dataset.loc[none_dataset_h.index.join(none_dataset_v.index , how='inner'), [category_col, 'HorizontalDimension', 'horizontal_'+category_col,  'VerticalDimension', 'vertical_'+category_col,]] = 'Undefined'\n",
    "none_dataset.drop_duplicates(inplace=True)\n",
    "del(none_dataset_h, none_dataset_v)\n",
    "\"\"\"\n",
    "none_dataset = df[(df.HorizontalDimension.fillna('None').isin([None, 'none', 'None'])) & (df.VerticalDimension.fillna('None').isin([None, 'none', 'None'])) & (df[['Libertarian', 'Authoritarian', 'Left', 'Right']].max(axis=1)<0.2) ]\n",
    "none_dataset.loc[none_dataset.index, [category_col, 'VerticalDimension', 'HorizontalDimension', 'horizontal_'+category_col, 'vertical_'+category_col, ]] = 'Undefined'\n",
    "\"\"\"\n",
    "\n",
    "#df.loc[none_dataset.index, :] = none_dataset.copy()\n",
    "curr_dataset = df[(df.HorizontalDimension==df.horizontal_polcompass_flair) | (df.VerticalDimension==df.vertical_polcompass_flair)]\n",
    "curr_dataset = pd.concat([curr_dataset, none_dataset], axis=0, ignore_index=False).sort_index()\n",
    "curr_dataset.loc[none_dataset.index, :] = none_dataset.copy()\n",
    "curr_dataset.drop_duplicates(inplace=True)\n",
    "if political_only:\n",
    "    curr_dataset = curr_dataset[curr_dataset.Political==True]\n",
    "#del(none_dataset)\n",
    "if (curr_dataset.index.value_counts()>1).any():\n",
    "    raise ValueError('Wrong index')\n",
    "curr_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Loaded orig...\n",
      "Loaded textual...\n",
      "Removed stopwords and non alpha tokens\n",
      "1\n",
      "Loaded orig...\n",
      "Loaded textual...\n",
      "Removed stopwords and non alpha tokens\n",
      "2\n",
      "Loaded orig...\n",
      "Loaded textual...\n",
      "Removed stopwords and non alpha tokens\n",
      "3\n",
      "Loaded orig...\n",
      "Loaded textual...\n",
      "Removed stopwords and non alpha tokens\n",
      "4\n",
      "Loaded orig...\n",
      "Loaded textual...\n",
      "Removed stopwords and non alpha tokens\n",
      "5\n",
      "Loaded orig...\n",
      "Loaded textual...\n",
      "Removed stopwords and non alpha tokens\n",
      "6\n",
      "Loaded orig...\n",
      "Loaded textual...\n",
      "Removed stopwords and non alpha tokens\n",
      "7\n",
      "Loaded orig...\n",
      "Loaded textual...\n",
      "Removed stopwords and non alpha tokens\n",
      "8\n",
      "Loaded orig...\n",
      "Loaded textual...\n",
      "Removed stopwords and non alpha tokens\n",
      "9\n",
      "Loaded orig...\n",
      "Loaded textual...\n",
      "Removed stopwords and non alpha tokens\n",
      "10\n",
      "Loaded orig...\n",
      "Loaded textual...\n",
      "Removed stopwords and non alpha tokens\n",
      "11\n",
      "Loaded orig...\n",
      "Loaded textual...\n",
      "Removed stopwords and non alpha tokens\n",
      "12\n",
      "Loaded orig...\n",
      "Loaded textual...\n",
      "Removed stopwords and non alpha tokens\n",
      "13\n",
      "Loaded orig...\n",
      "Loaded textual...\n",
      "Removed stopwords and non alpha tokens\n",
      "14\n",
      "Loaded orig...\n",
      "Loaded textual...\n",
      "Removed stopwords and non alpha tokens\n",
      "15\n",
      "Loaded orig...\n",
      "Loaded textual...\n",
      "Removed stopwords and non alpha tokens\n",
      "16\n",
      "Loaded orig...\n",
      "Loaded textual...\n",
      "Removed stopwords and non alpha tokens\n",
      "17\n",
      "Loaded orig...\n",
      "Loaded textual...\n",
      "Removed stopwords and non alpha tokens\n",
      "18\n",
      "Loaded orig...\n",
      "Loaded textual...\n",
      "Removed stopwords and non alpha tokens\n",
      "19\n",
      "Loaded orig...\n",
      "Loaded textual...\n",
      "Removed stopwords and non alpha tokens\n",
      "20\n",
      "Loaded orig...\n",
      "Loaded textual...\n",
      "Removed stopwords and non alpha tokens\n",
      "21\n",
      "Loaded orig...\n",
      "Loaded textual...\n",
      "Removed stopwords and non alpha tokens\n",
      "22\n",
      "Loaded orig...\n",
      "Loaded textual...\n",
      "Removed stopwords and non alpha tokens\n",
      "23\n",
      "Loaded orig...\n",
      "Loaded textual...\n",
      "Removed stopwords and non alpha tokens\n",
      "24\n",
      "Loaded orig...\n",
      "Loaded textual...\n",
      "Removed stopwords and non alpha tokens\n",
      "25\n",
      "Loaded orig...\n",
      "Loaded textual...\n",
      "Removed stopwords and non alpha tokens\n",
      "26\n",
      "Loaded orig...\n",
      "Loaded textual...\n",
      "Removed stopwords and non alpha tokens\n",
      "27\n",
      "Loaded orig...\n",
      "Loaded textual...\n",
      "Removed stopwords and non alpha tokens\n",
      "28\n",
      "Loaded orig...\n",
      "Loaded textual...\n",
      "Removed stopwords and non alpha tokens\n",
      "29\n",
      "Loaded orig...\n",
      "Loaded textual...\n",
      "Removed stopwords and non alpha tokens\n",
      "30\n",
      "Loaded orig...\n",
      "Loaded textual...\n",
      "Removed stopwords and non alpha tokens\n",
      "31\n",
      "Loaded orig...\n",
      "Loaded textual...\n",
      "Removed stopwords and non alpha tokens\n",
      "32\n",
      "Loaded orig...\n",
      "Loaded textual...\n",
      "Removed stopwords and non alpha tokens\n",
      "33\n",
      "Loaded orig...\n",
      "Loaded textual...\n",
      "Removed stopwords and non alpha tokens\n",
      "34\n",
      "Loaded orig...\n",
      "Loaded textual...\n",
      "Removed stopwords and non alpha tokens\n",
      "35\n",
      "Loaded orig...\n",
      "Loaded textual...\n",
      "Removed stopwords and non alpha tokens\n",
      "36\n",
      "Loaded orig...\n",
      "Loaded textual...\n",
      "Removed stopwords and non alpha tokens\n",
      "37\n",
      "Loaded orig...\n",
      "Loaded textual...\n",
      "Removed stopwords and non alpha tokens\n",
      "38\n",
      "Loaded orig...\n",
      "Loaded textual...\n",
      "Removed stopwords and non alpha tokens\n",
      "39\n",
      "Loaded orig...\n",
      "Loaded textual...\n",
      "Removed stopwords and non alpha tokens\n",
      "40\n",
      "Loaded orig...\n",
      "Loaded textual...\n",
      "Removed stopwords and non alpha tokens\n",
      "41\n",
      "Loaded orig...\n",
      "Loaded textual...\n",
      "Removed stopwords and non alpha tokens\n",
      "42\n",
      "Loaded orig...\n",
      "Loaded textual...\n",
      "Removed stopwords and non alpha tokens\n",
      "43\n",
      "Loaded orig...\n",
      "Loaded textual...\n",
      "Removed stopwords and non alpha tokens\n",
      "44\n",
      "Loaded orig...\n",
      "Loaded textual...\n",
      "Removed stopwords and non alpha tokens\n",
      "45\n",
      "Loaded orig...\n",
      "Loaded textual...\n",
      "Removed stopwords and non alpha tokens\n",
      "46\n",
      "Loaded orig...\n",
      "Loaded textual...\n",
      "Removed stopwords and non alpha tokens\n",
      "47\n",
      "Loaded orig...\n",
      "Loaded textual...\n",
      "Removed stopwords and non alpha tokens\n",
      "48\n",
      "Loaded orig...\n",
      "Loaded textual...\n",
      "Removed stopwords and non alpha tokens\n",
      "49\n",
      "Loaded orig...\n",
      "Loaded textual...\n",
      "Removed stopwords and non alpha tokens\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>date</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>filtered_tokens</th>\n",
       "      <th>polcompass_flair</th>\n",
       "      <th>horizontal_polcompass_flair</th>\n",
       "      <th>vertical_polcompass_flair</th>\n",
       "      <th>Political</th>\n",
       "      <th>HorizontalDimension</th>\n",
       "      <th>VerticalDimension</th>\n",
       "      <th>Populism</th>\n",
       "      <th>PeopleCentrism</th>\n",
       "      <th>AntiElitism</th>\n",
       "      <th>EmotionalAppeal</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>content_id</th>\n",
       "      <th>content_type</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>799133.0</th>\n",
       "      <th>comment</th>\n",
       "      <td>595</td>\n",
       "      <td>2020-07-09</td>\n",
       "      <td>2020-07-09 02:48:30</td>\n",
       "      <td>news</td>\n",
       "      <td>[jeez, people, complain, capitalism, free, mar...</td>\n",
       "      <td>libright</td>\n",
       "      <td>right</td>\n",
       "      <td>lib</td>\n",
       "      <td>True</td>\n",
       "      <td>right</td>\n",
       "      <td>lib</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>792191.0</th>\n",
       "      <th>comment</th>\n",
       "      <td>595</td>\n",
       "      <td>2020-08-03</td>\n",
       "      <td>2020-08-03 01:56:39</td>\n",
       "      <td>GoldandBlack</td>\n",
       "      <td>[student, pretty, crazy, though, denied, uyghu...</td>\n",
       "      <td>Undefined</td>\n",
       "      <td>Undefined</td>\n",
       "      <td>Undefined</td>\n",
       "      <td>True</td>\n",
       "      <td>Undefined</td>\n",
       "      <td>Undefined</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>794631.0</th>\n",
       "      <th>comment</th>\n",
       "      <td>595</td>\n",
       "      <td>2020-08-05</td>\n",
       "      <td>2020-08-05 23:16:13</td>\n",
       "      <td>CapitalismVSocialism</td>\n",
       "      <td>[state, regulation, actually, destroy, competi...</td>\n",
       "      <td>libright</td>\n",
       "      <td>right</td>\n",
       "      <td>lib</td>\n",
       "      <td>True</td>\n",
       "      <td>centrist</td>\n",
       "      <td>lib</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>888767.0</th>\n",
       "      <th>comment</th>\n",
       "      <td>671</td>\n",
       "      <td>2022-10-03</td>\n",
       "      <td>2022-10-03 02:45:43</td>\n",
       "      <td>TheLeftCantMeme</td>\n",
       "      <td>[page, trump, businessman, v]</td>\n",
       "      <td>centrist</td>\n",
       "      <td>centrist</td>\n",
       "      <td>centrist</td>\n",
       "      <td>True</td>\n",
       "      <td>right</td>\n",
       "      <td>centrist</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1957812.0</th>\n",
       "      <th>comment</th>\n",
       "      <td>1397</td>\n",
       "      <td>2021-08-04</td>\n",
       "      <td>2021-08-04 16:06:57</td>\n",
       "      <td>AskLibertarians</td>\n",
       "      <td>[however, mentally, physically, prepared, pare...</td>\n",
       "      <td>Undefined</td>\n",
       "      <td>Undefined</td>\n",
       "      <td>lib</td>\n",
       "      <td>True</td>\n",
       "      <td>Undefined</td>\n",
       "      <td>None</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48352281.0</th>\n",
       "      <th>comment</th>\n",
       "      <td>34768</td>\n",
       "      <td>2021-04-05</td>\n",
       "      <td>2021-04-05 03:00:04</td>\n",
       "      <td>monarchism</td>\n",
       "      <td>[unaware, term, used, describe, followers, hoppe]</td>\n",
       "      <td>libright</td>\n",
       "      <td>right</td>\n",
       "      <td>lib</td>\n",
       "      <td>True</td>\n",
       "      <td>right</td>\n",
       "      <td>lib</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49115586.0</th>\n",
       "      <th>comment</th>\n",
       "      <td>35380</td>\n",
       "      <td>2020-09-24</td>\n",
       "      <td>2020-09-24 22:59:45</td>\n",
       "      <td>Libertarian</td>\n",
       "      <td>[oh, boy, #x200b, male-woman, marriage, consen...</td>\n",
       "      <td>authright</td>\n",
       "      <td>right</td>\n",
       "      <td>auth</td>\n",
       "      <td>True</td>\n",
       "      <td>right</td>\n",
       "      <td>lib</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49707478.0</th>\n",
       "      <th>comment</th>\n",
       "      <td>35854</td>\n",
       "      <td>2022-04-04</td>\n",
       "      <td>2022-04-04 13:10:31</td>\n",
       "      <td>CapitalismVSocialism</td>\n",
       "      <td>[argument, could, possibly, provide, someone, ...</td>\n",
       "      <td>libleft</td>\n",
       "      <td>left</td>\n",
       "      <td>lib</td>\n",
       "      <td>True</td>\n",
       "      <td>left</td>\n",
       "      <td>lib</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49587578.0</th>\n",
       "      <th>comment</th>\n",
       "      <td>35763</td>\n",
       "      <td>2020-11-03</td>\n",
       "      <td>2020-11-03 18:33:46</td>\n",
       "      <td>AskALiberal</td>\n",
       "      <td>[europe, nobody, know, reasons, vote, trump, h...</td>\n",
       "      <td>libcenter</td>\n",
       "      <td>centrist</td>\n",
       "      <td>lib</td>\n",
       "      <td>True</td>\n",
       "      <td>centrist</td>\n",
       "      <td>centrist</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49611632.0</th>\n",
       "      <th>comment</th>\n",
       "      <td>35781</td>\n",
       "      <td>2020-01-20</td>\n",
       "      <td>2020-01-20 00:33:16</td>\n",
       "      <td>ModelUSElections</td>\n",
       "      <td>[list, gop, assembly, party, gop, district, st...</td>\n",
       "      <td>libright</td>\n",
       "      <td>right</td>\n",
       "      <td>lib</td>\n",
       "      <td>True</td>\n",
       "      <td>right</td>\n",
       "      <td>auth</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>584 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                         author       date         created_utc  \\\n",
       "content_id content_type                                          \n",
       "799133.0   comment          595 2020-07-09 2020-07-09 02:48:30   \n",
       "792191.0   comment          595 2020-08-03 2020-08-03 01:56:39   \n",
       "794631.0   comment          595 2020-08-05 2020-08-05 23:16:13   \n",
       "888767.0   comment          671 2022-10-03 2022-10-03 02:45:43   \n",
       "1957812.0  comment         1397 2021-08-04 2021-08-04 16:06:57   \n",
       "...                         ...        ...                 ...   \n",
       "48352281.0 comment        34768 2021-04-05 2021-04-05 03:00:04   \n",
       "49115586.0 comment        35380 2020-09-24 2020-09-24 22:59:45   \n",
       "49707478.0 comment        35854 2022-04-04 2022-04-04 13:10:31   \n",
       "49587578.0 comment        35763 2020-11-03 2020-11-03 18:33:46   \n",
       "49611632.0 comment        35781 2020-01-20 2020-01-20 00:33:16   \n",
       "\n",
       "                                    subreddit  \\\n",
       "content_id content_type                         \n",
       "799133.0   comment                       news   \n",
       "792191.0   comment               GoldandBlack   \n",
       "794631.0   comment       CapitalismVSocialism   \n",
       "888767.0   comment            TheLeftCantMeme   \n",
       "1957812.0  comment            AskLibertarians   \n",
       "...                                       ...   \n",
       "48352281.0 comment                 monarchism   \n",
       "49115586.0 comment                Libertarian   \n",
       "49707478.0 comment       CapitalismVSocialism   \n",
       "49587578.0 comment                AskALiberal   \n",
       "49611632.0 comment           ModelUSElections   \n",
       "\n",
       "                                                           filtered_tokens  \\\n",
       "content_id content_type                                                      \n",
       "799133.0   comment       [jeez, people, complain, capitalism, free, mar...   \n",
       "792191.0   comment       [student, pretty, crazy, though, denied, uyghu...   \n",
       "794631.0   comment       [state, regulation, actually, destroy, competi...   \n",
       "888767.0   comment                           [page, trump, businessman, v]   \n",
       "1957812.0  comment       [however, mentally, physically, prepared, pare...   \n",
       "...                                                                    ...   \n",
       "48352281.0 comment       [unaware, term, used, describe, followers, hoppe]   \n",
       "49115586.0 comment       [oh, boy, #x200b, male-woman, marriage, consen...   \n",
       "49707478.0 comment       [argument, could, possibly, provide, someone, ...   \n",
       "49587578.0 comment       [europe, nobody, know, reasons, vote, trump, h...   \n",
       "49611632.0 comment       [list, gop, assembly, party, gop, district, st...   \n",
       "\n",
       "                        polcompass_flair horizontal_polcompass_flair  \\\n",
       "content_id content_type                                                \n",
       "799133.0   comment              libright                       right   \n",
       "792191.0   comment             Undefined                   Undefined   \n",
       "794631.0   comment              libright                       right   \n",
       "888767.0   comment              centrist                    centrist   \n",
       "1957812.0  comment             Undefined                   Undefined   \n",
       "...                                  ...                         ...   \n",
       "48352281.0 comment              libright                       right   \n",
       "49115586.0 comment             authright                       right   \n",
       "49707478.0 comment               libleft                        left   \n",
       "49587578.0 comment             libcenter                    centrist   \n",
       "49611632.0 comment              libright                       right   \n",
       "\n",
       "                        vertical_polcompass_flair Political  \\\n",
       "content_id content_type                                       \n",
       "799133.0   comment                            lib      True   \n",
       "792191.0   comment                      Undefined      True   \n",
       "794631.0   comment                            lib      True   \n",
       "888767.0   comment                       centrist      True   \n",
       "1957812.0  comment                            lib      True   \n",
       "...                                           ...       ...   \n",
       "48352281.0 comment                            lib      True   \n",
       "49115586.0 comment                           auth      True   \n",
       "49707478.0 comment                            lib      True   \n",
       "49587578.0 comment                            lib      True   \n",
       "49611632.0 comment                            lib      True   \n",
       "\n",
       "                        HorizontalDimension VerticalDimension  Populism  \\\n",
       "content_id content_type                                                   \n",
       "799133.0   comment                    right               lib       0.3   \n",
       "792191.0   comment                Undefined         Undefined       0.1   \n",
       "794631.0   comment                 centrist               lib       0.1   \n",
       "888767.0   comment                    right          centrist       0.1   \n",
       "1957812.0  comment                Undefined              None       0.1   \n",
       "...                                     ...               ...       ...   \n",
       "48352281.0 comment                    right               lib       0.1   \n",
       "49115586.0 comment                    right               lib       0.1   \n",
       "49707478.0 comment                     left               lib       0.6   \n",
       "49587578.0 comment                 centrist          centrist       0.3   \n",
       "49611632.0 comment                    right              auth       0.1   \n",
       "\n",
       "                         PeopleCentrism  AntiElitism  EmotionalAppeal  \n",
       "content_id content_type                                                \n",
       "799133.0   comment                  0.4          0.5              0.3  \n",
       "792191.0   comment                  0.1          0.1              0.3  \n",
       "794631.0   comment                  0.2          0.4              0.2  \n",
       "888767.0   comment                  0.1          0.1              0.2  \n",
       "1957812.0  comment                  0.1          0.1              0.7  \n",
       "...                                 ...          ...              ...  \n",
       "48352281.0 comment                  0.2          0.3              0.1  \n",
       "49115586.0 comment                  0.3          0.2              0.4  \n",
       "49707478.0 comment                  0.7          0.8              0.7  \n",
       "49587578.0 comment                  0.4          0.5              0.3  \n",
       "49611632.0 comment                  0.2          0.1              0.1  \n",
       "\n",
       "[584 rows x 15 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils import pandas_io_handler\n",
    "from myText import *\n",
    "from text_replacement import replace_features_in_text\n",
    "from nltk.corpus import stopwords\n",
    "import random\n",
    "import pandas as pd \n",
    "\n",
    "eng_stopwords = stopwords.words('english')\n",
    "author_sample_frac = 1 ## percentage of authors to load from each chunk\n",
    "min_author_n_of_texts = 1\n",
    "content_types = ['post','comment']\n",
    "comment_ids = curr_dataset[curr_dataset.index.get_level_values('content_type')=='comment'].index.get_level_values('content_id')\n",
    "post_ids = curr_dataset[curr_dataset.index.get_level_values('content_type')=='post'].index.get_level_values('content_id')\n",
    "final_df = pd.DataFrame()\n",
    "for i in range(50):\n",
    "    try:\n",
    "        if sorted(final_df.index)==sorted(curr_dataset.index):\n",
    "            break\n",
    "        common_starting_file = './data/dataset_whole/mapped_dataframes/{}_chunk_{}_{}.parquet'.format('{}', i, '{}')\n",
    "        print(i)\n",
    "        ##ORIGINAL DF (author, author_flair, subreddit, date...)\n",
    "        curr_final_df = pd.concat([\n",
    "            pd.read_parquet(common_starting_file.format(content+'s', 'orig_new'), columns=['author', 'date', 'created_utc', 'subreddit'], \n",
    "                            filters=[('{}_id'.format(content), 'in', eval('{}_ids'.format(content))),\n",
    "                                    ]).reset_index()  \n",
    "                    for content in content_types], axis=0, ignore_index=True)\n",
    "        if curr_final_df.empty:\n",
    "            continue\n",
    "        if (author_sample_frac < 1) or (min_author_n_of_texts > 1):\n",
    "            n_of_authors_to_load = int(round (curr_final_df.author.nunique() * min([1, author_sample_frac]), 0))\n",
    "            curr_final_df = curr_final_df.groupby(\"author\").filter(lambda x: x['post_id'].count() > min_author_n_of_texts)\n",
    "            sampled_authors = pd.Series(curr_final_df.author.unique()).sample(min(curr_final_df.author.nunique(), n_of_authors_to_load)).values\n",
    "            curr_final_df = curr_final_df[curr_final_df.author.isin(sampled_authors)]\n",
    "            del(sampled_authors)\n",
    "        print('Loaded orig...')\n",
    "        curr_final_df['content_id'] = curr_final_df['post_id'].fillna(curr_final_df['comment_id'])\n",
    "        curr_final_df['content_type'] = np.nan\n",
    "        curr_final_df['content_type'] = curr_final_df['content_type'].fillna(curr_final_df.loc[curr_final_df.post_id.notna(), 'post_id'].map(lambda t: 'post')).fillna('comment')\n",
    "        \n",
    "        textual_df = pd.concat([pd.read_parquet(common_starting_file.format(content+'s', 'textualfiltered_new'), \n",
    "                                                filters=[('{}_id'.format(content), 'in', curr_final_df['{}_id'.format(content)].values)]).reset_index() \n",
    "                                for content in content_types], axis=0, ignore_index=True)    \n",
    "        textual_df['content_id'] = textual_df['post_id'].fillna(textual_df['comment_id'])\n",
    "        textual_df['content_type'] = np.nan\n",
    "        textual_df['content_type'] = textual_df['content_type'].fillna(textual_df.loc[textual_df.post_id.notna(), 'post_id'].map(lambda t: 'post')).fillna('comment')\n",
    "        textual_df.drop(columns=['post_id', 'comment_id'], inplace=True)\n",
    "        print('Loaded textual...')\n",
    "        \n",
    "        curr_final_df = curr_final_df.drop(columns=['post_id', 'comment_id'], errors='ignore').merge(textual_df, how='inner', on=['content_id', 'content_type'])\n",
    "        \n",
    "        ## FILTERING TOKENS (REMOVING URLs, EMOJIS, EMOTICONS, MENTIONS, PUNCTUATION, ALL NON-ALPHA CHARS) \n",
    "        ## FILTERING FOR EACH FILE IN ORDER TO SAVE MEMORY (AVOIDING CONCATENATING WHOLE TEXTUAL DF BUT ONLY KEEPING VALID TOKENS)\n",
    "        filtered_tokens = curr_final_df.apply(lambda t: MyText(t, text_col='text'),axis=1).apply(lambda t: replace_features_in_text(t, text_col='tokens', columns_to_replace=[], columns_to_remove=['urls','emoticons','emojis','mentions','repeatedPunctuation'])).map(lambda tokens: [t for token in tokens if (t:=token.lower()) not in eng_stopwords and t.islower()])\n",
    "        print('Removed stopwords and non alpha tokens')\n",
    "        curr_final_df['filtered_tokens'] = filtered_tokens\n",
    "        final_df = pd.concat([final_df, curr_final_df.set_index(['content_id', 'content_type']).drop(columns=textual_df.columns, errors='ignore')], axis=0, ignore_index=False)\n",
    "        del(curr_final_df, textual_df, filtered_tokens)\n",
    "    except FileNotFoundError as e:\n",
    "        print(e)\n",
    "        continue\n",
    "    \n",
    "final_df = final_df.join(curr_dataset.loc[:, ['polcompass_flair','horizontal_polcompass_flair','vertical_polcompass_flair','Political','HorizontalDimension','VerticalDimension','Populism','PeopleCentrism','AntiElitism','EmotionalAppeal']], how='inner')\n",
    "final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: xlabel='vertical_polcompass_flair'>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x600 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from author_flair_mapping import get_mother_categories\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axs = plt.subplots(1,3, figsize=(20,6))\n",
    "\n",
    "final_df.loc[(final_df['VerticalDimension'].fillna('centrist')==final_df['vertical_polcompass_flair'].fillna('centrist')) & (final_df['HorizontalDimension'].fillna('centrist')==final_df['horizontal_polcompass_flair'].fillna('centrist')), category_col].value_counts(dropna=False).plot(kind='bar', ax=axs[0])\n",
    "\n",
    "final_df.loc[(final_df.HorizontalDimension==final_df.horizontal_polcompass_flair), 'horizontal_'+category_col].value_counts().plot(kind='bar', ax=axs[1])\n",
    "\n",
    "final_df.loc[(final_df.VerticalDimension==final_df.vertical_polcompass_flair), 'vertical_'+category_col].value_counts().plot(kind='bar', ax=axs[2])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import author_flair_mapping\n",
    "import warnings\n",
    "from imblearn.over_sampling import SMOTE\n",
    "    \n",
    "def get_curr_dataset(df: pd.DataFrame, dimension: str, training_col: str = 'filtered_tokens', category_col: str = 'author_flair_text', category_ovo: str = None, undersample: str = None, force_if_already_existing: bool = False, values_only: bool = False, split_x_y: bool = None, dropna: bool = True):\n",
    "    if split_x_y is None:\n",
    "        split_x_y = values_only\n",
    "        \n",
    "    dimension = author_flair_mapping.__map_to_valid_dimension__(dimension)\n",
    "    curr_y_col = category_col if dimension is None else dimension+'_'+category_col\n",
    "         \n",
    "    if dimension in [author_flair_mapping.VERTICAL_DIMENSION, author_flair_mapping.HORIZONTAL_DIMENSION]:\n",
    "        df = map_mother_categories(df.copy(), category_col) if ( curr_y_col not in df.columns or force_if_already_existing ) else df.copy()\n",
    "    \n",
    "    if isinstance(training_col, (np.ndarray, pd.Series, pd.Index)):\n",
    "        training_col = list(training_col)\n",
    "    \n",
    "    df = df[flatten([training_col])+[curr_y_col]]\n",
    "    dataset = df[df.notna().all(axis=1)] if dropna else df[df[curr_y_col].notna()]\n",
    "\n",
    "    if category_ovo:\n",
    "        valid_flairs = author_flair_mapping.vertical_valid_flairs if dimension==author_flair_mapping.VERTICAL_DIMENSION else author_flair_mapping.horizontal_valid_flairs if dimension==author_flair_mapping.HORIZONTAL_DIMENSION else author_flair_mapping.all_valid_flairs \n",
    "        if category_ovo not in valid_flairs:\n",
    "            warnings.warn('The category \"{}\" is not a valid category for the current dimension \"{}\"... Will still map to binary but it is very likely not to have any matches (i.e. all the binary class values will be False)'.format(category_ovo, dimension))\n",
    "        tmp_s = dataset.loc[:, curr_y_col].copy().map(lambda v: v==category_ovo)\n",
    "        dataset[curr_y_col+'_is_'+category_ovo] = tmp_s.copy()\n",
    "        curr_y_col += '_is_'+category_ovo\n",
    "    if undersample:\n",
    "        if undersample.lower()=='smote':\n",
    "            classes_freq = dataset[curr_y_col].value_counts()\n",
    "            resampled_dct = {k:new_v for (k,v) in classes_freq.tail(-1).to_dict().items() if (new_v:=min(classes_freq.loc[k]*20, round(classes_freq.iloc[0]*1)))>v}\n",
    "            smote = SMOTE(random_state=42, sampling_strategy=resampled_dct)\n",
    "            X_resampled, y_resampled = smote.fit_resample(dataset[training_col].values, dataset[curr_y_col].values)\n",
    "            dataset = pd.concat([pd.DataFrame(X_resampled, columns=training_col), pd.Series(y_resampled, name=curr_y_col)], axis=1, ignore_index=False)\n",
    "        else:\n",
    "            if undersample.lower()=='max':\n",
    "                n_of_samples_by_category = int(round(len(dataset)/dataset[curr_y_col].nunique(), 0))\n",
    "            else:\n",
    "                n_of_samples_by_category = dataset[curr_y_col].value_counts().min()\n",
    "            dataset = dataset.groupby(curr_y_col).apply(\\\n",
    "                                  lambda x:x.sample(min(len(x), n_of_samples_by_category)),include_groups=False)\\\n",
    "                                    .reset_index(level=0).loc[:,dataset.columns]\n",
    "    if not split_x_y:\n",
    "        if not isinstance(training_col, (list, np.ndarray, pd.Series, pd.Index)):\n",
    "            training_col = [training_col]\n",
    "        return dataset[training_col+[curr_y_col]].values if values_only else dataset[training_col+[curr_y_col]]\n",
    "    else:\n",
    "        return (dataset[training_col].values, dataset[curr_y_col].values) if values_only else (dataset[training_col], dataset[curr_y_col])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_Sw_15ILDQ8F"
   },
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEPS:\n",
    "    1. Estimating best parameters for tf-idf (min df, best k, ngram_range)\n",
    "    2. Calculating/generating TF-IDF vectors\n",
    "    3. Eventually grouping at author level\n",
    "    4. Adding other features\n",
    "    5. Building Model(s)\n",
    "        5.1 Estimating best params for the current model through CV and grid_search\n",
    "    6. Combining models (Horizontal-> Vertical; Vertical -> Horizontal)\n",
    "        6.1 1vsOthers vs Multiclass Model\n",
    "    7. Comparing results (1 Model vs H+V vs V+H; 1vsO vs MultiLabel Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 3674,
     "status": "ok",
     "timestamp": 1731679583633,
     "user": {
      "displayName": "LAURA POLLACCI",
      "userId": "16888075574530779678"
     },
     "user_tz": -60
    },
    "id": "PuGY_RkLIVfA"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import LinearSVC, SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "laoDhVVG9ieo"
   },
   "source": [
    "## SKLEARN PIPELINE -> TFIDF, CHI-SQUARE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Default pipeline, on text level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from author_flair_mapping import map_mother_categories\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "curr_dimension = None\n",
    "global_label_encoder = LabelEncoder()\n",
    "global_label_encoder.fit_transform(final_df[category_col].dropna())\n",
    "global_label_mapping = {c:global_label_encoder.transform([c])[0] for c in global_label_encoder.classes_}\n",
    "\n",
    "h_label_encoder = LabelEncoder()\n",
    "h_label_encoder.fit_transform(final_df['horizontal_'+category_col].dropna())\n",
    "h_label_mapping = {c:h_label_encoder.transform([c])[0] for c in h_label_encoder.classes_}\n",
    "\n",
    "v_label_encoder = LabelEncoder()\n",
    "v_label_encoder.fit_transform(final_df['vertical_'+category_col].dropna())\n",
    "v_label_mapping = {c:v_label_encoder.transform([c])[0] for c in v_label_encoder.classes_}\n",
    "\n",
    "\n",
    "inv_global_label_mapping = {v:k for k,v in global_label_mapping.items()}\n",
    "inv_h_label_mapping = {v:k for k,v in h_label_mapping.items()}\n",
    "inv_v_label_mapping = {v:k for k,v in v_label_mapping.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_col = 'filtered_tokens'\n",
    "curr_category_one_vs_others = None\n",
    "curr_dimension = None\n",
    "remove_hashtags_from_token = True\n",
    "\n",
    "if remove_hashtags_from_token:\n",
    "    final_df[text_col] = final_df[text_col].map(lambda tokens: [tok if not tok.lower().startswith('#') else tok.replace('#','') for tok in tokens if tok!='#x200b' ])\n",
    "\n",
    "\n",
    "x_train, y_train =  get_curr_dataset(final_df, dimension=curr_dimension, training_col=text_col, category_col=category_col, split_x_y=True, values_only=False)\n",
    "\n",
    "#y_train_bin = pd.Series(y_train).str.lower()==curr_category_one_vs_others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 24066,
     "status": "ok",
     "timestamp": 1731679607697,
     "user": {
      "displayName": "LAURA POLLACCI",
      "userId": "16888075574530779678"
     },
     "user_tz": -60
    },
    "id": "Ka6cKclx9ieo",
    "outputId": "80edc9bb-6d76-4449-f17f-7eaf6a8def6d"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tfidf_utils import do_nothing\n",
    "\n",
    "\n",
    "best_preprocess_params_pipeline = Pipeline([\n",
    "    ('vectorizer', CountVectorizer(tokenizer=do_nothing, preprocessor=do_nothing)),  # frequencies\n",
    "    ('tfidf', TfidfTransformer()),  # tfidf\n",
    "    ('kbest', SelectKBest(score_func=chi2)),  # feature/words selection\n",
    "    ('learner', OneVsRestClassifier(LinearSVC()) )  # learning algorithm\n",
    "])\n",
    "\n",
    "y_train_numeric = global_label_encoder.transform(y_train)\n",
    "\n",
    "param_grid = {\n",
    "    'vectorizer__min_df': [.00001, .0001, .001], \n",
    "    'vectorizer__ngram_range': [(1, 1), (1,2), (1, 3)], \n",
    "    'kbest__k': [1000],  # Numero di feature da selezionare\n",
    "}\n",
    "\n",
    "# Creare il GridSearchCV con DummyClassifier (senza un vero classificatore)\n",
    "best_preprocess_grid_search = GridSearchCV(best_preprocess_params_pipeline, param_grid, cv=5, n_jobs=10, verbose=3)\n",
    "best_preprocess_grid_search.fit(x_train.sample(500),y_train_numeric[:500])\n",
    "best_preprocess_grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kb = best_preprocess_grid_search.best_estimator_['kbest']#.get_features_names_out()\n",
    "kb_indexes = list(map(lambda v: int(v.split('x')[1]), kb.get_feature_names_out()))\n",
    "best_preprocess_grid_search.best_estimator_['vectorizer'].get_feature_names_out()[kb_indexes]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### USING BEST PARAMS TO BUILD A TF-IDF MATRIX\n",
    "#### SPLITTING TRAINING AND TEST SET AND BUILDING/TESTING MODELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = final_df[final_df.author.isin(\n",
    "            final_df[final_df[category_col].notna()].author.unique() )]\n",
    "\n",
    "\n",
    "test_sample_frac = 0.2\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from author_flair_mapping import map_mother_categories\n",
    "\n",
    "split_by_authors = False\n",
    "if split_by_authors:\n",
    "    train,test = [curr_dataset.explode(final_df.drop(columns=['date','created_utc','subreddit', ], errors='ignore')).reset_index().set_index(['author']+final_df.index.names) for curr_dataset in train_test_split(final_df.reset_index().groupby('author')[final_df.index.names+[text_col, category_col]].agg(lambda t: list(t)), test_size=test_sample_frac)]#.set_index('author')#[['author','filtered_tokens','author_flair_text']]\n",
    "else:\n",
    "    train,test=train_test_split(final_df.reset_index().set_index(['author']+final_df.index.names).drop(columns=['date','created_utc','subreddit',] , errors='ignore'), test_size=test_sample_frac)    \n",
    "\n",
    "del(final_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import author_flair_mapping\n",
    "curr_category_one_vs_others = None\n",
    "curr_dimension = None\n",
    "\n",
    "text_col, curr_y_col  = 'filtered_tokens', 'polcompass_flair'\n",
    "\n",
    "x_train, y_train, x_test, y_test = train[text_col].values, train[curr_y_col].values, test[text_col].values, test[curr_y_col].values\n",
    "y_train_bin = pd.Series(y_train).str.lower()==curr_category_one_vs_others"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CALCULATING TF-IDF AND ESTIMATING BEST PARAMETERS (NGRAMS, BEST K ETC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "def get_best_params_and_scores_gridsearch(gridsearch: GridSearchCV) -> pd.DataFrame:\n",
    "    sorted_scores = sorted([(i, v) for i,v in enumerate(gridsearch.cv_results_['mean_test_score'])], key=lambda x: x[1], reverse=True)\n",
    "    sorted_best_params = [(i, gridsearch.cv_results_['params'][i]) for i in map(lambda x: x[0], sorted_scores)]\n",
    "    \n",
    "    return pd.merge(pd.DataFrame.from_records(sorted_scores, columns=['index', 'mean_test_score']).set_index('index'),\n",
    "           pd.DataFrame.from_records(sorted_best_params, columns=['index', 'parameters']).set_index('index').parameters.apply(pd.Series),\n",
    "             left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "grid_search_params_linearsvc = [\n",
    "    {'C': [0.1, 1, 10], 'class_weight': [None, 'balanced'], 'loss': ['hinge', 'squared_hinge'],\n",
    "     'penalty': ['l2']},\n",
    "    {'C': [0.1, 1, 10], 'class_weight': [None, 'balanced'], 'loss': ['squared_hinge'],\n",
    "     'penalty': ['l1']},\n",
    "\n",
    "]\n",
    "\n",
    "grid_search_params_randomforest = [{'criterion': ['gini', 'entropy'], 'n_estimators': [10, 100],\n",
    "                                   'min_samples_split': [0.001, 0.02], 'class_weight': [None, 'balanced']}\n",
    "                                  ]\n",
    "\n",
    "grid_search_params_logregression = [{'penalty': ['l2'], 'solver': ['sag', 'saga'],\n",
    "                                     'class_weight': [None, 'balanced'], 'C': [0.1, 1, 10],\n",
    "                                    'max_iter': [5000, 1000]},\n",
    "                                   {'penalty': [None], 'solver': ['sag', 'saga'],\n",
    "                                    'class_weight': [None, 'balanced'], 'max_iter': [5000, 1000]}]\n",
    "\n",
    "\n",
    "estimator_to_params_dict = {LinearSVC(max_iter=3000, dual='auto'): grid_search_params_linearsvc,\n",
    "                            RandomForestClassifier(): grid_search_params_randomforest,\n",
    "                           LogisticRegression(tol=1e-3): grid_search_params_logregression}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "best_score = 1 # 1 for best, 2 for 2nd best, etc\n",
    "best_index = np.where(best_preprocess_grid_search.cv_results_['rank_test_score']==best_score)[0][0]\n",
    "\n",
    "tfidf_pipeline = Pipeline([\n",
    "    ('vectorizer', CountVectorizer(tokenizer=do_nothing, preprocessor=do_nothing, ngram_range=(1,3))),  # frequencies\n",
    "    ('tfidf', TfidfTransformer()),  # tfidf\n",
    "    ('kbest', SelectKBest(score_func=chi2, k=20)),\n",
    "])\n",
    "tfidf_pipeline.set_params(**best_preprocess_grid_search.cv_results_['params'][best_index])\n",
    "#tfidf_pipeline.set_params(**{'kbest__k':100})\n",
    "xtrain_tfidf = tfidf_pipeline.fit_transform(x_train, y_train)\n",
    "train['tfidf'] = np.array(csr_matrix(xtrain_tfidf)) #list(xtrain_tfidf.toarray())\n",
    "\n",
    "xtest_tfidf = tfidf_pipeline.transform(x_test)\n",
    "test['tfidf'] = np.array(csr_matrix(xtest_tfidf)) #list(xtrain_tfidf.toarray())\n",
    "\n",
    "del(xtrain_tfidf, xtest_tfidf, x_train, y_train, x_test, y_test)\n",
    "train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BUILDING MODELS ON TF-IDF FEATURES ONLY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1077,
     "status": "ok",
     "timestamp": 1731679608768,
     "user": {
      "displayName": "LAURA POLLACCI",
      "userId": "16888075574530779678"
     },
     "user_tz": -60
    },
    "id": "iPU-T_iY9ieq",
    "outputId": "b2ffd39f-5a09-479c-9697-f28109d08479"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "#from sklearn.model_selection import ParameterGrid\n",
    "from scipy.sparse import csr_matrix\n",
    "from scipy.sparse import hstack as scipy_hstack\n",
    "from scipy.sparse import vstack as scipy_vstack\n",
    "import time\n",
    "curr_train_col = 'tfidf'\n",
    "curr_dimension = None\n",
    "curr_train = get_curr_dataset(train, dimension=curr_dimension, training_col=curr_train_col, category_col=category_col).sample(frac=0.1)\n",
    "curr_train_col, curr_y_col = curr_train.columns\n",
    "\n",
    "y_encoded = global_label_encoder.transform(curr_train[curr_y_col])  # Codifica il target (le etichette)\n",
    "estimators = {}\n",
    "for estimator, grid_search_params in estimator_to_params_dict.items():\n",
    "    print(estimator)\n",
    "    time.sleep(3)\n",
    "    whole_clf_gridsearch = GridSearchCV(estimator, n_jobs=6, cv=3, param_grid=grid_search_params, verbose=3)\n",
    "    whole_clf_gridsearch.fit(scipy_vstack(curr_train[curr_train_col].values), y_encoded)    \n",
    "    predictions = whole_clf_gridsearch.predict(scipy_vstack(test[curr_train_col]))\n",
    "    print('Classification report:')\n",
    "    print(classification_report(global_label_encoder.transform(test[curr_y_col]), predictions))\n",
    "    print('Confusion matrix:')\n",
    "    cm = confusion_matrix(global_label_encoder.transform(test[curr_y_col]), predictions)\n",
    "    print(cm)\n",
    "    \n",
    "    estimators[str(estimator).split('()')[0]] = get_best_params_and_scores_gridsearch(whole_clf_gridsearch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "curr_dimension = 'h'\n",
    "curr_train = get_curr_dataset(train, dimension=curr_dimension, training_col=curr_train_col, category_col=category_col).sample(frac=0.2)\n",
    "curr_test = get_curr_dataset(test, dimension=curr_dimension, training_col=curr_train_col, category_col=category_col)\n",
    "curr_train_col, curr_y_col = curr_train.columns\n",
    "\n",
    "y_encoded = h_label_encoder.transform(curr_train[curr_y_col])  # Codifica il target (le etichette)\n",
    "h_estimators = {}\n",
    "for estimator, grid_search_params in estimator_to_params_dict.items():\n",
    "    print(estimator)\n",
    "    time.sleep(3)\n",
    "    horizontal_clf_gridsearch = GridSearchCV(estimator, n_jobs=6, cv=3, param_grid=grid_search_params, verbose=3)\n",
    "    horizontal_clf_gridsearch.fit(scipy_vstack(curr_train[curr_train_col].values), y_encoded)    \n",
    "    predictions = horizontal_clf_gridsearch.predict(scipy_vstack(curr_test[curr_train_col]))\n",
    "    print('Classification report (dimension {}, classifier {}):'.format(curr_dimension, estimator))\n",
    "    print(classification_report(h_label_encoder.transform(curr_test[curr_y_col]), predictions))\n",
    "    print('Confusion matrix:')\n",
    "    cm = confusion_matrix(h_label_encoder.transform(curr_test[curr_y_col]), predictions)\n",
    "    print(cm)\n",
    "    \n",
    "    h_estimators[str(estimator).split('()')[0]] = get_best_params_and_scores_gridsearch(horizontal_clf_gridsearch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "curr_dimension = 'v'\n",
    "curr_train = get_curr_dataset(train, dimension=curr_dimension, training_col=curr_train_col, category_col=category_col).sample(frac=0.2)\n",
    "curr_test = get_curr_dataset(test, dimension=curr_dimension, training_col=curr_train_col, category_col=category_col)\n",
    "curr_train_col, curr_y_col = curr_train.columns\n",
    "\n",
    "y_encoded = v_label_encoder.transform(curr_train[curr_y_col])  # Codifica il target (le etichette)\n",
    "v_estimators = {}\n",
    "for estimator, grid_search_params in estimator_to_params_dict.items():\n",
    "    print(estimator)\n",
    "    time.sleep(3)\n",
    "    vertical_clf_gridsearch = GridSearchCV(estimator, n_jobs=6, cv=3, param_grid=grid_search_params, verbose=3)\n",
    "    vertical_clf_gridsearch.fit(scipy_vstack(curr_train[curr_train_col].values), y_encoded)\n",
    "    predictions = vertical_clf_gridsearch.predict(scipy_vstack(curr_test[curr_train_col]))\n",
    "    print('Classification report (dimension {}, classifier {}):'.format(curr_dimension, estimator))\n",
    "    print(classification_report(v_label_encoder.transform(curr_test[curr_y_col]), predictions))\n",
    "    print('Confusion matrix:')\n",
    "    cm = confusion_matrix(v_label_encoder.transform(curr_test[curr_y_col]), predictions)\n",
    "    print(cm)\n",
    "    \n",
    "    v_estimators[str(estimator).split('()')[0]] = get_best_params_and_scores_gridsearch(vertical_clf_gridsearch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### BUILDING MODEL WITH 'BALANCED' PARAMETER FOR CLASS WEIGHT\n",
    "\n",
    "curr_dimension = 'v'\n",
    "curr_train = get_curr_dataset(train, dimension=curr_dimension, training_col=curr_train_col, category_col=category_col).sample(frac=0.2)\n",
    "curr_test = get_curr_dataset(test, dimension=curr_dimension, training_col=curr_train_col, category_col=category_col)\n",
    "curr_train_col, curr_y_col = curr_train.columns\n",
    "\n",
    "y_encoded = v_label_encoder.transform(curr_train[curr_y_col])  # Codifica il target (le etichette)\n",
    "\n",
    "\n",
    "v_rf = RandomForestClassifier(min_samples_split=.01, n_estimators=10, class_weight='balanced')\n",
    "v_rf.fit(scipy_vstack(curr_train[curr_train_col].values), y_encoded)\n",
    "#curr_train\n",
    "\n",
    "predictions = v_rf.predict(scipy_vstack(curr_test[curr_train_col]))\n",
    "print('Classification report (dimension {}, classifier {}):'.format(curr_dimension, estimator))\n",
    "print(classification_report(v_label_encoder.transform(curr_test[curr_y_col]), predictions))\n",
    "print('Confusion matrix:')\n",
    "cm = confusion_matrix(v_label_encoder.transform(curr_test[curr_y_col]), predictions)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_score = 37 # 1 for best, 2 for 2nd best, etc\n",
    "best_index = np.where(vertical_clf_gridsearch.cv_results_['rank_test_score']==best_score)[0][0]\n",
    "vertical_clf_gridsearch.cv_results_['params'][best_index]\n",
    "#vertical_clf_gridsearch.cv_results_['mean_test_score'][best_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ADDING CONTEXTUAL FEATURES (sentiment, VAD, emotional features etc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "textual_df.apply(lambda row: filter_list(list_to_filter=list(map(lambda x: x[0],row['pos_tag'])),\n",
    "                                        list_to_look=row['filtered_tokens'],\n",
    "                                        return_indexes=True), axis=1)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_from_disk = False\n",
    "#curr_saving_directory = '../Tesi/code/data/models/8/'\n",
    "if load_from_disk:\n",
    "    train = pd.read_parquet(curr_saving_directory+'train.parquet')\n",
    "    test = pd.read_parquet(curr_saving_directory+'test.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "executionInfo": {
     "elapsed": 343,
     "status": "ok",
     "timestamp": 1731679843797,
     "user": {
      "displayName": "LAURA POLLACCI",
      "userId": "16888075574530779678"
     },
     "user_tz": -60
    },
    "id": "MPSyxGPzKRhm",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/dataset_whole/mapped_dataframes/posts_chunk_0_{}.parquet\n",
      "./data/dataset_whole/mapped_dataframes/comments_chunk_0_{}.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\onest\\AppData\\Local\\Temp\\ipykernel_10084\\2823162311.py:33: UserWarning: No data loaded, probably none of them is in the current training/test set...\n",
      "  warnings.warn('No data loaded, probably none of them is in the current training/test set...')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/dataset_whole/mapped_dataframes/posts_chunk_1_{}.parquet\n",
      "./data/dataset_whole/mapped_dataframes/comments_chunk_1_{}.parquet\n",
      "./data/dataset_whole/mapped_dataframes/posts_chunk_2_{}.parquet\n",
      "./data/dataset_whole/mapped_dataframes/comments_chunk_2_{}.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\onest\\AppData\\Local\\Temp\\ipykernel_10084\\2823162311.py:33: UserWarning: No data loaded, probably none of them is in the current training/test set...\n",
      "  warnings.warn('No data loaded, probably none of them is in the current training/test set...')\n",
      "C:\\Users\\onest\\AppData\\Local\\Temp\\ipykernel_10084\\2823162311.py:33: UserWarning: No data loaded, probably none of them is in the current training/test set...\n",
      "  warnings.warn('No data loaded, probably none of them is in the current training/test set...')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/dataset_whole/mapped_dataframes/posts_chunk_3_{}.parquet\n",
      "./data/dataset_whole/mapped_dataframes/comments_chunk_3_{}.parquet\n",
      "./data/dataset_whole/mapped_dataframes/posts_chunk_4_{}.parquet\n",
      "./data/dataset_whole/mapped_dataframes/comments_chunk_4_{}.parquet\n",
      "./data/dataset_whole/mapped_dataframes/posts_chunk_5_{}.parquet\n",
      "./data/dataset_whole/mapped_dataframes/comments_chunk_5_{}.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\onest\\AppData\\Local\\Temp\\ipykernel_10084\\2823162311.py:33: UserWarning: No data loaded, probably none of them is in the current training/test set...\n",
      "  warnings.warn('No data loaded, probably none of them is in the current training/test set...')\n",
      "C:\\Users\\onest\\AppData\\Local\\Temp\\ipykernel_10084\\2823162311.py:33: UserWarning: No data loaded, probably none of them is in the current training/test set...\n",
      "  warnings.warn('No data loaded, probably none of them is in the current training/test set...')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/dataset_whole/mapped_dataframes/posts_chunk_6_{}.parquet\n",
      "./data/dataset_whole/mapped_dataframes/comments_chunk_6_{}.parquet\n",
      "./data/dataset_whole/mapped_dataframes/posts_chunk_7_{}.parquet\n",
      "./data/dataset_whole/mapped_dataframes/comments_chunk_7_{}.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\onest\\AppData\\Local\\Temp\\ipykernel_10084\\2823162311.py:33: UserWarning: No data loaded, probably none of them is in the current training/test set...\n",
      "  warnings.warn('No data loaded, probably none of them is in the current training/test set...')\n",
      "C:\\Users\\onest\\AppData\\Local\\Temp\\ipykernel_10084\\2823162311.py:33: UserWarning: No data loaded, probably none of them is in the current training/test set...\n",
      "  warnings.warn('No data loaded, probably none of them is in the current training/test set...')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/dataset_whole/mapped_dataframes/posts_chunk_8_{}.parquet\n",
      "./data/dataset_whole/mapped_dataframes/comments_chunk_8_{}.parquet\n",
      "./data/dataset_whole/mapped_dataframes/posts_chunk_9_{}.parquet\n",
      "./data/dataset_whole/mapped_dataframes/comments_chunk_9_{}.parquet\n",
      "./data/dataset_whole/mapped_dataframes/posts_chunk_10_{}.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\onest\\AppData\\Local\\Temp\\ipykernel_10084\\2823162311.py:33: UserWarning: No data loaded, probably none of them is in the current training/test set...\n",
      "  warnings.warn('No data loaded, probably none of them is in the current training/test set...')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/dataset_whole/mapped_dataframes/comments_chunk_10_{}.parquet\n",
      "./data/dataset_whole/mapped_dataframes/posts_chunk_11_{}.parquet\n",
      "./data/dataset_whole/mapped_dataframes/comments_chunk_11_{}.parquet\n",
      "./data/dataset_whole/mapped_dataframes/posts_chunk_12_{}.parquet\n",
      "./data/dataset_whole/mapped_dataframes/comments_chunk_12_{}.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\onest\\AppData\\Local\\Temp\\ipykernel_10084\\2823162311.py:33: UserWarning: No data loaded, probably none of them is in the current training/test set...\n",
      "  warnings.warn('No data loaded, probably none of them is in the current training/test set...')\n",
      "C:\\Users\\onest\\AppData\\Local\\Temp\\ipykernel_10084\\2823162311.py:33: UserWarning: No data loaded, probably none of them is in the current training/test set...\n",
      "  warnings.warn('No data loaded, probably none of them is in the current training/test set...')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/dataset_whole/mapped_dataframes/posts_chunk_13_{}.parquet\n",
      "./data/dataset_whole/mapped_dataframes/comments_chunk_13_{}.parquet\n",
      "./data/dataset_whole/mapped_dataframes/posts_chunk_14_{}.parquet\n",
      "./data/dataset_whole/mapped_dataframes/comments_chunk_14_{}.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\onest\\AppData\\Local\\Temp\\ipykernel_10084\\2823162311.py:33: UserWarning: No data loaded, probably none of them is in the current training/test set...\n",
      "  warnings.warn('No data loaded, probably none of them is in the current training/test set...')\n",
      "C:\\Users\\onest\\AppData\\Local\\Temp\\ipykernel_10084\\2823162311.py:33: UserWarning: No data loaded, probably none of them is in the current training/test set...\n",
      "  warnings.warn('No data loaded, probably none of them is in the current training/test set...')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/dataset_whole/mapped_dataframes/posts_chunk_15_{}.parquet\n",
      "./data/dataset_whole/mapped_dataframes/comments_chunk_15_{}.parquet\n",
      "./data/dataset_whole/mapped_dataframes/posts_chunk_16_{}.parquet\n",
      "./data/dataset_whole/mapped_dataframes/comments_chunk_16_{}.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\onest\\AppData\\Local\\Temp\\ipykernel_10084\\2823162311.py:33: UserWarning: No data loaded, probably none of them is in the current training/test set...\n",
      "  warnings.warn('No data loaded, probably none of them is in the current training/test set...')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/dataset_whole/mapped_dataframes/posts_chunk_17_{}.parquet\n",
      "./data/dataset_whole/mapped_dataframes/comments_chunk_17_{}.parquet\n",
      "./data/dataset_whole/mapped_dataframes/posts_chunk_18_{}.parquet\n",
      "./data/dataset_whole/mapped_dataframes/comments_chunk_18_{}.parquet\n",
      "./data/dataset_whole/mapped_dataframes/posts_chunk_19_{}.parquet\n",
      "./data/dataset_whole/mapped_dataframes/comments_chunk_19_{}.parquet\n",
      "./data/dataset_whole/mapped_dataframes/posts_chunk_20_{}.parquet\n",
      "./data/dataset_whole/mapped_dataframes/comments_chunk_20_{}.parquet\n",
      "./data/dataset_whole/mapped_dataframes/posts_chunk_21_{}.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\onest\\AppData\\Local\\Temp\\ipykernel_10084\\2823162311.py:33: UserWarning: No data loaded, probably none of them is in the current training/test set...\n",
      "  warnings.warn('No data loaded, probably none of them is in the current training/test set...')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/dataset_whole/mapped_dataframes/comments_chunk_21_{}.parquet\n",
      "./data/dataset_whole/mapped_dataframes/posts_chunk_22_{}.parquet\n",
      "./data/dataset_whole/mapped_dataframes/comments_chunk_22_{}.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\onest\\AppData\\Local\\Temp\\ipykernel_10084\\2823162311.py:33: UserWarning: No data loaded, probably none of them is in the current training/test set...\n",
      "  warnings.warn('No data loaded, probably none of them is in the current training/test set...')\n",
      "C:\\Users\\onest\\AppData\\Local\\Temp\\ipykernel_10084\\2823162311.py:33: UserWarning: No data loaded, probably none of them is in the current training/test set...\n",
      "  warnings.warn('No data loaded, probably none of them is in the current training/test set...')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/dataset_whole/mapped_dataframes/posts_chunk_23_{}.parquet\n",
      "./data/dataset_whole/mapped_dataframes/comments_chunk_23_{}.parquet\n",
      "./data/dataset_whole/mapped_dataframes/posts_chunk_24_{}.parquet\n",
      "./data/dataset_whole/mapped_dataframes/comments_chunk_24_{}.parquet\n",
      "./data/dataset_whole/mapped_dataframes/posts_chunk_25_{}.parquet\n",
      "./data/dataset_whole/mapped_dataframes/comments_chunk_25_{}.parquet\n",
      "./data/dataset_whole/mapped_dataframes/posts_chunk_26_{}.parquet\n",
      "./data/dataset_whole/mapped_dataframes/comments_chunk_26_{}.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\onest\\AppData\\Local\\Temp\\ipykernel_10084\\2823162311.py:33: UserWarning: No data loaded, probably none of them is in the current training/test set...\n",
      "  warnings.warn('No data loaded, probably none of them is in the current training/test set...')\n",
      "C:\\Users\\onest\\AppData\\Local\\Temp\\ipykernel_10084\\2823162311.py:33: UserWarning: No data loaded, probably none of them is in the current training/test set...\n",
      "  warnings.warn('No data loaded, probably none of them is in the current training/test set...')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/dataset_whole/mapped_dataframes/posts_chunk_27_{}.parquet\n",
      "./data/dataset_whole/mapped_dataframes/comments_chunk_27_{}.parquet\n",
      "./data/dataset_whole/mapped_dataframes/posts_chunk_28_{}.parquet\n",
      "./data/dataset_whole/mapped_dataframes/comments_chunk_28_{}.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\onest\\AppData\\Local\\Temp\\ipykernel_10084\\2823162311.py:33: UserWarning: No data loaded, probably none of them is in the current training/test set...\n",
      "  warnings.warn('No data loaded, probably none of them is in the current training/test set...')\n",
      "C:\\Users\\onest\\AppData\\Local\\Temp\\ipykernel_10084\\2823162311.py:33: UserWarning: No data loaded, probably none of them is in the current training/test set...\n",
      "  warnings.warn('No data loaded, probably none of them is in the current training/test set...')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/dataset_whole/mapped_dataframes/posts_chunk_29_{}.parquet\n",
      "./data/dataset_whole/mapped_dataframes/comments_chunk_29_{}.parquet\n",
      "./data/dataset_whole/mapped_dataframes/posts_chunk_30_{}.parquet\n",
      "./data/dataset_whole/mapped_dataframes/comments_chunk_30_{}.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\onest\\AppData\\Local\\Temp\\ipykernel_10084\\2823162311.py:33: UserWarning: No data loaded, probably none of them is in the current training/test set...\n",
      "  warnings.warn('No data loaded, probably none of them is in the current training/test set...')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/dataset_whole/mapped_dataframes/posts_chunk_31_{}.parquet\n",
      "./data/dataset_whole/mapped_dataframes/comments_chunk_31_{}.parquet\n",
      "./data/dataset_whole/mapped_dataframes/posts_chunk_32_{}.parquet\n",
      "./data/dataset_whole/mapped_dataframes/comments_chunk_32_{}.parquet\n",
      "./data/dataset_whole/mapped_dataframes/posts_chunk_33_{}.parquet\n",
      "./data/dataset_whole/mapped_dataframes/comments_chunk_33_{}.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\onest\\AppData\\Local\\Temp\\ipykernel_10084\\2823162311.py:33: UserWarning: No data loaded, probably none of them is in the current training/test set...\n",
      "  warnings.warn('No data loaded, probably none of them is in the current training/test set...')\n",
      "C:\\Users\\onest\\AppData\\Local\\Temp\\ipykernel_10084\\2823162311.py:33: UserWarning: No data loaded, probably none of them is in the current training/test set...\n",
      "  warnings.warn('No data loaded, probably none of them is in the current training/test set...')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/dataset_whole/mapped_dataframes/posts_chunk_34_{}.parquet\n",
      "./data/dataset_whole/mapped_dataframes/comments_chunk_34_{}.parquet\n",
      "./data/dataset_whole/mapped_dataframes/posts_chunk_35_{}.parquet\n",
      "./data/dataset_whole/mapped_dataframes/comments_chunk_35_{}.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\onest\\AppData\\Local\\Temp\\ipykernel_10084\\2823162311.py:33: UserWarning: No data loaded, probably none of them is in the current training/test set...\n",
      "  warnings.warn('No data loaded, probably none of them is in the current training/test set...')\n",
      "C:\\Users\\onest\\AppData\\Local\\Temp\\ipykernel_10084\\2823162311.py:33: UserWarning: No data loaded, probably none of them is in the current training/test set...\n",
      "  warnings.warn('No data loaded, probably none of them is in the current training/test set...')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/dataset_whole/mapped_dataframes/posts_chunk_36_{}.parquet\n",
      "./data/dataset_whole/mapped_dataframes/comments_chunk_36_{}.parquet\n",
      "./data/dataset_whole/mapped_dataframes/posts_chunk_37_{}.parquet\n",
      "./data/dataset_whole/mapped_dataframes/comments_chunk_37_{}.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\onest\\AppData\\Local\\Temp\\ipykernel_10084\\2823162311.py:33: UserWarning: No data loaded, probably none of them is in the current training/test set...\n",
      "  warnings.warn('No data loaded, probably none of them is in the current training/test set...')\n",
      "C:\\Users\\onest\\AppData\\Local\\Temp\\ipykernel_10084\\2823162311.py:33: UserWarning: No data loaded, probably none of them is in the current training/test set...\n",
      "  warnings.warn('No data loaded, probably none of them is in the current training/test set...')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/dataset_whole/mapped_dataframes/posts_chunk_38_{}.parquet\n",
      "./data/dataset_whole/mapped_dataframes/comments_chunk_38_{}.parquet\n",
      "./data/dataset_whole/mapped_dataframes/posts_chunk_39_{}.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\onest\\AppData\\Local\\Temp\\ipykernel_10084\\2823162311.py:33: UserWarning: No data loaded, probably none of them is in the current training/test set...\n",
      "  warnings.warn('No data loaded, probably none of them is in the current training/test set...')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/dataset_whole/mapped_dataframes/comments_chunk_39_{}.parquet\n",
      "./data/dataset_whole/mapped_dataframes/posts_chunk_40_{}.parquet\n",
      "./data/dataset_whole/mapped_dataframes/comments_chunk_40_{}.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\onest\\AppData\\Local\\Temp\\ipykernel_10084\\2823162311.py:33: UserWarning: No data loaded, probably none of them is in the current training/test set...\n",
      "  warnings.warn('No data loaded, probably none of them is in the current training/test set...')\n",
      "C:\\Users\\onest\\AppData\\Local\\Temp\\ipykernel_10084\\2823162311.py:33: UserWarning: No data loaded, probably none of them is in the current training/test set...\n",
      "  warnings.warn('No data loaded, probably none of them is in the current training/test set...')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/dataset_whole/mapped_dataframes/posts_chunk_41_{}.parquet\n",
      "./data/dataset_whole/mapped_dataframes/comments_chunk_41_{}.parquet\n",
      "./data/dataset_whole/mapped_dataframes/posts_chunk_42_{}.parquet\n",
      "./data/dataset_whole/mapped_dataframes/comments_chunk_42_{}.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\onest\\AppData\\Local\\Temp\\ipykernel_10084\\2823162311.py:33: UserWarning: No data loaded, probably none of them is in the current training/test set...\n",
      "  warnings.warn('No data loaded, probably none of them is in the current training/test set...')\n",
      "C:\\Users\\onest\\AppData\\Local\\Temp\\ipykernel_10084\\2823162311.py:33: UserWarning: No data loaded, probably none of them is in the current training/test set...\n",
      "  warnings.warn('No data loaded, probably none of them is in the current training/test set...')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/dataset_whole/mapped_dataframes/posts_chunk_43_{}.parquet\n",
      "./data/dataset_whole/mapped_dataframes/comments_chunk_43_{}.parquet\n",
      "./data/dataset_whole/mapped_dataframes/posts_chunk_44_{}.parquet\n",
      "./data/dataset_whole/mapped_dataframes/comments_chunk_44_{}.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\onest\\AppData\\Local\\Temp\\ipykernel_10084\\2823162311.py:33: UserWarning: No data loaded, probably none of them is in the current training/test set...\n",
      "  warnings.warn('No data loaded, probably none of them is in the current training/test set...')\n",
      "C:\\Users\\onest\\AppData\\Local\\Temp\\ipykernel_10084\\2823162311.py:33: UserWarning: No data loaded, probably none of them is in the current training/test set...\n",
      "  warnings.warn('No data loaded, probably none of them is in the current training/test set...')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/dataset_whole/mapped_dataframes/posts_chunk_45_{}.parquet\n",
      "./data/dataset_whole/mapped_dataframes/comments_chunk_45_{}.parquet\n",
      "./data/dataset_whole/mapped_dataframes/posts_chunk_46_{}.parquet\n",
      "./data/dataset_whole/mapped_dataframes/comments_chunk_46_{}.parquet\n",
      "./data/dataset_whole/mapped_dataframes/posts_chunk_47_{}.parquet\n",
      "./data/dataset_whole/mapped_dataframes/comments_chunk_47_{}.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\onest\\AppData\\Local\\Temp\\ipykernel_10084\\2823162311.py:33: UserWarning: No data loaded, probably none of them is in the current training/test set...\n",
      "  warnings.warn('No data loaded, probably none of them is in the current training/test set...')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/dataset_whole/mapped_dataframes/posts_chunk_48_{}.parquet\n",
      "./data/dataset_whole/mapped_dataframes/comments_chunk_48_{}.parquet\n",
      "./data/dataset_whole/mapped_dataframes/posts_chunk_49_{}.parquet\n",
      "./data/dataset_whole/mapped_dataframes/comments_chunk_49_{}.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\onest\\AppData\\Local\\Temp\\ipykernel_10084\\2823162311.py:33: UserWarning: No data loaded, probably none of them is in the current training/test set...\n",
      "  warnings.warn('No data loaded, probably none of them is in the current training/test set...')\n"
     ]
    }
   ],
   "source": [
    "from utils import pandas_io_handler\n",
    "from text_utils import get_wordnet_pos\n",
    "from textual_features_extraction import get_emoji_sentiment, get_emoticon_sentiment\n",
    "from utils import filter_list\n",
    "from nltk.corpus import wordnet\n",
    "\"\"\"\n",
    "TODO -> \n",
    "    2. FILLNA VS NON FILLNA FOR MEAN IN VAD AND SOCIALNESS (SO FAR MEAN=mean(SCORES_MATCHES) , if fillna MEAN=mean(all_tokens_scores) -> i.e. token_score=0 (default_value) if no match) \n",
    "\"\"\"\n",
    "curr_content_ind = np.where([x=='content_type' for x in train.index.names])[0][0]\n",
    "whole_index_len = len(train.index.names)\n",
    "content_locator_str = '[' + ':, ' * (curr_content_ind) + '\"{}\",' + ':, ' * (whole_index_len-curr_content_ind-1) + ']'\n",
    "unique_authors_str = \"set(train.index.get_level_values('author')) | set(test.index.get_level_values('author'))\"\n",
    "unique_posts_str = \"set(train.loc\"+content_locator_str.format(\"post\")+\".index.get_level_values('content_id')) | set(test.loc\"+content_locator_str.format(\"post\")+\".index.get_level_values('content_id'))\"\n",
    "unique_comments_str = \"set(train.loc\"+content_locator_str.format(\"comment\")+\".index.get_level_values('content_id')) | set(test.loc\"+content_locator_str.format(\"comment\")+\".index.get_level_values('content_id'))\"\n",
    "\n",
    "\n",
    "groupby_author=False\n",
    "content_types = ['posts','comments']\n",
    "features_df = pd.DataFrame()\n",
    "tmp = pd.concat([train[['filtered_tokens']],test[['filtered_tokens']]],axis=0,ignore_index=False).reset_index().set_index(['content_type','content_id'])[['filtered_tokens']]\n",
    "for i in range(50):\n",
    "    common_starting_file = './data/dataset_whole/mapped_dataframes/{}_chunk_{}_{}.parquet'.format('{}', i, '{}')\n",
    "    content_final_df = pd.DataFrame()\n",
    "    for content in content_types:\n",
    "        starting_file = common_starting_file.format(content, '{}')\n",
    "        print(starting_file)\n",
    "        \n",
    "        ##TEXTUAL DF (emojis, emoticons, urls, tokens, sentences...)\n",
    "        textual_df = pd.read_parquet(starting_file.format('textualfiltered_new'), filters=[('{}_id'.format(content[:-1]), 'in', eval(eval(\"unique_{}_str\".format(content))) )])\n",
    "        \n",
    "        if not len(textual_df):\n",
    "            warnings.warn('No data loaded, probably none of them is in the current training/test set...')\n",
    "            continue\n",
    "        \n",
    "        ##ORIGINAL DF (author, author_flair, subreddit, date...)\n",
    "        orig_df = pd.read_parquet(starting_file.format('orig_new'), columns=['author'], filters=[('{}_id'.format(content[:-1]), 'in', textual_df.index)])\n",
    "        #orig_df.sort_values(by=['author','created_utc','subreddit'], inplace=True)\n",
    "        #orig_df['author_flair_text'] = orig_df['author_flair_text'].map(lambda value: value if value>=0 else np.nan)\n",
    "        orig_df['content_type'] = content[:-1]\n",
    "\n",
    "        ###SPLITTING POLCOMPASS AND NON_POLCOMPASS \n",
    "        others_df = orig_df \n",
    "        #curr_polcompass_df = orig_df[orig_df.subreddit.str.lower().isin(['politicalcompass','politicalcompassmemes', 'politicalcompassmemes2'])]\n",
    "        #others_df = orig_df[~orig_df.index.isin(curr_polcompass_df.index)]\n",
    "        ###filling author_flair_text on the non_polcompass \n",
    "        #last_flairs_by_author = get_author_flair(polcompass_df.sort_values(by=['created_utc']).groupby('author'))#.apply(get_author_flair)\n",
    "        #others_df['author_flair_text'] = others_df[['author']].merge(last_flairs_by_author, left_on='author', right_index=True).author_flair_text\n",
    "\n",
    "        ###mapping textual_df to counts -> counting urls, upperWords, mentions, hashtags, badwords, emoticons, emojis, positive/negative emoticons|emojis, tokens, sentences\n",
    "        emojis_counts = pd.json_normalize(textual_df.emojis\\\n",
    "                                            .map(lambda emos: [get_emoji_sentiment(e['feature_value']) for e in emos])\\\n",
    "                                            .map(lambda sentiments: {sent: sentiments.count(sent) for sent in [-1,1,0]}))\\\n",
    "                            .rename({\n",
    "                                1:'positive_emojis_count', \n",
    "                                -1:'negative_emojis_count',\n",
    "                                0: 'neutral_emojis_count'},axis=1).set_index(textual_df.index)\n",
    "        emojis_counts['emojis_count'] = emojis_counts['positive_emojis_count']+emojis_counts['negative_emojis_count']+emojis_counts['neutral_emojis_count']\n",
    "\n",
    "        emoticons_counts = pd.json_normalize(textual_df.emoticons\\\n",
    "                                            .map(lambda emos: [get_emoticon_sentiment(e['feature_value']) for e in emos])\\\n",
    "                                            .map(lambda sentiments: {sent: sentiments.count(sent) for sent in [-1,1,0]}))\\\n",
    "                            .rename({\n",
    "                                1:'positive_emoticons_count', \n",
    "                                -1:'negative_emoticons_count',\n",
    "                                0: 'neutral_emoticons_count'},axis=1).set_index(textual_df.index)\n",
    "        emoticons_counts['emoticons_count'] = emoticons_counts['negative_emoticons_count']+emoticons_counts['positive_emoticons_count']+emoticons_counts['neutral_emoticons_count']\n",
    "        curr_textual_ind, curr_textual_cols = textual_df.index.name, textual_df.columns\n",
    "        textual_df['content_type'] = content[:-1]\n",
    "        textual_df = textual_df.reset_index().rename({'{}_id'.format(content[:-1]):'content_id'},axis=1).set_index(['content_type', 'content_id']).join(tmp['filtered_tokens'], on=['content_type', 'content_id'], how='inner')\n",
    "        textual_df['pos_tag'] = textual_df.apply(lambda row: [row['pos_tag'][ind] \n",
    "                              for ind in filter_list(list_to_filter=list(map(lambda x: x[0].lower() if not x[0].startswith('#') else x[0].replace('#','').lower(), row['pos_tag'])),\n",
    "                                        list_to_look=row['filtered_tokens'],\n",
    "                                        return_indexes=True)], axis=1)\n",
    "        #print(textual_df[textual_df.pos_tag.map(len)!=textual_df.filtered_tokens.map(len)].empty)\n",
    "        textual_df = textual_df.reset_index().rename({'content_id':'{}_id'.format(content[:-1])},axis=1)[list(curr_textual_cols)+flatten([curr_textual_ind])].set_index(curr_textual_ind)\n",
    "        tags_counts = pd.json_normalize(textual_df.pos_tag.map(lambda tags: [get_wordnet_pos(tag) for word,tag in tags])\\\n",
    "                                       .map(lambda tags: {tag: tags.count(tag) for tag in [wordnet.NOUN, wordnet.ADJ, wordnet.VERB, wordnet.ADV]}))\\\n",
    "                        .rename({\n",
    "                            wordnet.NOUN:'names_count', \n",
    "                            wordnet.ADJ:'adjectives_count',\n",
    "                            wordnet.VERB: 'verbs_count',\n",
    "                            wordnet.ADV: 'adverbs_count'},axis=1).set_index(textual_df.index)\n",
    "        counts_df = textual_df.drop(['text','clean_text', 'emojis', 'emoticons', 'pos_tag'],axis=1).map(lambda c: len(c)).rename(lambda c: c+'_count',axis=1)\n",
    "        ## mapping counts to bool (text either contains a feature (eg an emoji) or not)\n",
    "        #presence_absence_df = counts_df.drop(columns=['tokens_count', 'sentences_count']).map(lambda c: int(bool(c))).rename(lambda c: '_bool'.join(c.split('_count')), axis=1)\n",
    "\n",
    "        textual_df = pd.concat(\n",
    "            [textual_df[['text','clean_text']], counts_df, emojis_counts, emoticons_counts, tags_counts],\n",
    "            axis=1, ignore_index=False)\n",
    "        #textual_df = pd.concat([textual_df, presence_absence_df], axis=1, ignore_index=False)\n",
    "        #del(presence_absence_df)\n",
    "        del(counts_df, emojis_counts, emoticons_counts, tags_counts)\n",
    "\n",
    "        ###joining original_df and textual_df (only on non_polcompass texts)\n",
    "        others_df = others_df.join(textual_df, on='{}_id'.format(content[:-1]))\n",
    "        del(textual_df, orig_df)\n",
    "\n",
    "        ###SOCIAL, VAD AND NRCLEX \n",
    "        ###EXTRACTING MEAN(mean valence, mean arousal, mean socialness etc) OF THE MATCHES\n",
    "        ###EXTRACTING N OF STRONG MATCHES / N OF TOKENS\n",
    "        social_df = pandas_io_handler(starting_file.format('socialness_new'), filters=[('{}_id'.format(content[:-1]), 'in', others_df.index)])\n",
    "        social_df['strong_socialness_tokens_ratio'] = ((social_df['socialness_strong_tokens'] * (social_df['num_matched_tokens']/social_df['num_matches']))/ social_df['num_tokens'] ).fillna(0)\n",
    "        social_df = social_df[['strong_socialness_tokens_ratio', 'socialness_mean']]\n",
    "        social_cols = list(social_df.columns)\n",
    "\n",
    "        vad_df = pandas_io_handler(starting_file.format('vad_new'), filters=[('{}_id'.format(content[:-1]), 'in', others_df.index)])\n",
    "        vad_cols = []\n",
    "        for feat in ['valence', 'arousal', 'dominance']:\n",
    "            vad_df['strong_{}_tokens_ratio'.format(feat)] = ((vad_df['{}_strong_matches'.format(feat)] * (vad_df['num_matched_tokens']/vad_df['num_matches']))/ vad_df['num_tokens'] ).fillna(0)\n",
    "            vad_cols.extend(['strong_{}_tokens_ratio'.format(feat), '{}_mean'.format(feat)])\n",
    "\n",
    "        vad_df = vad_df[vad_cols]\n",
    "\n",
    "        nrclex_df = pandas_io_handler(starting_file.format('nrclex_new'), filters=[('{}_id'.format(content[:-1]), 'in', others_df.index)])\n",
    "        nrclex_df = nrclex_df.drop('num_tokens',axis=1).div(nrclex_df['num_tokens'], axis=0)\n",
    "        nrclex_cols = list(nrclex_df.columns)\n",
    "\n",
    "\n",
    "        ###SENTIMENT AND ENTITIES \n",
    "        def get_engspacy_res(curr_res_dct, weight_by_neutral=False):\n",
    "            if 'negative' not in curr_res_dct or 'positive' not in curr_res_dct:\n",
    "                raise ValueError('negative and positive must be in the current result')\n",
    "            positive, negative = curr_res_dct['positive'], curr_res_dct['negative']\n",
    "            abs_sentiment = positive-negative\n",
    "            if not weight_by_neutral:\n",
    "                return abs_sentiment\n",
    "            neutral = curr_res_dct['neutral'] if 'neutral' in curr_res_dct else 1-(positive+negative)\n",
    "            return abs_sentiment*(1-neutral)\n",
    "\n",
    "        def get_spacy_dct(spacy_entities_dct, join_others_keys=True, drop=False):\n",
    "            numerical_keys = ['CARDINAL','ORDINAL','PERCENT','QUANTITY']\n",
    "            time_keys = ['TIME', 'DATE']\n",
    "            keys_to_join = {'NUMERICAL': numerical_keys,\n",
    "                            'TIME': time_keys}\n",
    "            if drop or join_others_keys:\n",
    "                new_dct = {k:v if v is not None else [] for k,v in spacy_entities_dct.items() if k not in numerical_keys+time_keys}\n",
    "                if drop:\n",
    "                    return new_dct\n",
    "            if not join_others_keys:\n",
    "                return {k:v if v is not None else [] for k,v in spacy_entities_dct.items()}\n",
    "\n",
    "            for new_k in keys_to_join.keys():\n",
    "                new_dct[new_k] = sum([],flatten([curr_value if (curr_value:=spacy_entities_dct[k]) is not None else [] for k in keys_to_join[new_k]]))\n",
    "            return new_dct\n",
    "\n",
    "        \n",
    "        contextual_df = pandas_io_handler(starting_file.format('features_new'), filters=[('{}_id'.format(content[:-1]), 'in', others_df.index)]).drop(columns=['textblob_sentiment', 'textblob_subjectivity'], errors='ignore')\n",
    "        contextual_df['eng_spacysentiment'] = contextual_df.eng_spacysentiment.map(lambda v: get_engspacy_res(v, weight_by_neutral=True))\n",
    "\n",
    "        ##mapping entities to counts\n",
    "        contextual_df['nltk_entities'] = contextual_df['nltk_entities'].map(lambda all_values: {k:v if v is not None else [] for k,v in all_values.items() })\n",
    "        contextual_df['nltk_entities_count'] = contextual_df.nltk_entities.map(lambda all_values: sum([len(e) for e in all_values.values()]))\n",
    "        all_nltk_entities_types = contextual_df.nltk_entities.map(lambda t: t.keys()).explode().unique()\n",
    "        for entity_type in all_nltk_entities_types:\n",
    "            contextual_df['nltk_{}_count'.format(entity_type)] =  contextual_df.nltk_entities.map(lambda all_values: len(all_values[entity_type]))    \n",
    "            #contextual_df['nltk_{}_bool'.format(entity_type)] =  contextual_df['nltk_{}_count'.format(entity_type)].map(bool).astype(int)\n",
    "\n",
    "        contextual_df['spacy_entities'] = contextual_df['spacy_entities'].map(get_spacy_dct)\n",
    "        contextual_df['spacy_entities_count'] = contextual_df.spacy_entities.map(lambda all_values: sum([len(e) for e in all_values.values()]))\n",
    "        all_spacy_entities_types = contextual_df.spacy_entities.map(lambda t: t.keys()).explode().unique()\n",
    "        for entity_type in all_spacy_entities_types:\n",
    "            contextual_df['spacy_{}_count'.format(entity_type)] =  contextual_df.spacy_entities.map(lambda all_values: len(all_values[entity_type]))\n",
    "            #contextual_df['spacy_{}_bool'.format(entity_type)] =  contextual_df['spacy_{}_count'.format(entity_type)].map(bool).astype(int)\n",
    "        contextual_df = contextual_df.drop(['nltk_entities', 'spacy_entities'],axis=1)\n",
    "\n",
    "        \n",
    "        ### FRAMEAXIS (EMFD SCORES)\n",
    "        frameaxis_df = pandas_io_handler(starting_file.format('frameaxis_new'), filters=[('{}_id'.format(content[:-1]), 'in', others_df.index)])\n",
    "        \n",
    "        ### DETOXIFY\n",
    "        detoxify_df = pandas_io_handler(starting_file.format('detoxify_new'), filters=[('{}_id'.format(content[:-1]), 'in', others_df.index)])\n",
    "\n",
    "        \n",
    "        ###JOINING ALL FEATURES TOGETHER\n",
    "        others_df = others_df.join(\n",
    "                pd.concat([social_df, vad_df, nrclex_df, frameaxis_df, detoxify_df, contextual_df], axis=1, ignore_index=False),\n",
    "            on='{}_id'.format(content[:-1]))\n",
    "        del(social_df, vad_df, nrclex_df, frameaxis_df, detoxify_df, contextual_df)\n",
    "\n",
    "        ### NORMALIZING ALL THE \"COUNT\" FEATURES BY N OF TOKENS \n",
    "        counts_cols_to_normalize = [c for c in others_df.columns if c.endswith('_count') and c not in ['tokens_count', 'sentences_count']]\n",
    "        \n",
    "        others_df[counts_cols_to_normalize] = others_df[counts_cols_to_normalize].div(others_df['tokens_count'], axis=0)    \n",
    "        postag_cols = ['names_count', 'adjectives_count', 'verbs_count', 'adverbs_count']\n",
    "        others_df[postag_cols] = others_df[postag_cols].apply(lambda row: row / rowsum if (rowsum:=row.sum())>0 else row, axis=1)\n",
    "\n",
    "        content_final_df = pd.concat([content_final_df, others_df.reset_index().rename({'{}_id'.format(content[:-1]):'content_id'},axis=1) ], axis=0, ignore_index=True)\n",
    "        del(others_df)\n",
    "        \n",
    "    if not content_final_df.empty:\n",
    "        content_final_df.set_index(['content_id','content_type'],inplace=True)\n",
    "        if groupby_author:\n",
    "            grpd_df = content_final_df.reset_index().drop(['content_id', 'date', 'created_utc', 'subreddit',\n",
    "               'text', 'clean_text'],axis=1).groupby('author').agg(lambda x: x.value_counts().to_dict() if x.name in ['content_type', 'author_flair_text'] else x.sum() if x.name.endswith('_bool') else x.mean())\n",
    "            \n",
    "        features_df = pd.concat([features_df, grpd_df if groupby_author else content_final_df],axis=0, ignore_index=False)\n",
    "        del(content_final_df) \n",
    "\n",
    "del(tmp)\n",
    "whole_count_cols = ['emoticons_count', 'emojis_count', 'nltk_entities_count', 'spacy_entities_count']\n",
    "subcat_count_cols = [c for c in features_df.columns if c.endswith('_count') and c not in whole_count_cols+['sentences_count','tokens_count']]\n",
    "subcat_count_cols = {c:c.split('_')[0]+'_entities_count' if c.startswith(('nltk_','spacy_')) else 'emoticons_count' if 'emoticons_count' in c else 'emojis_count' if 'emojis_count' in c else c for c in subcat_count_cols}\n",
    "\n",
    "vice_cols, virtue_cols =[c for c in features_df.columns if c.endswith('.vice')], [c for c in features_df.columns if c.endswith('.virtue')]\n",
    "bias_cols = ['bias_loyalty', 'bias_care', 'bias_sanctity', 'bias_authority', 'bias_fairness']\n",
    "intensity_cols = ['intensity_loyalty', 'intensity_care', 'intensity_sanctity', 'intensity_authority', 'intensity_fairness']\n",
    "detoxify_cols = ['detoxify_toxicity', 'detoxify_severe_toxicity', 'detoxify_obscene', 'detoxify_identity_attack', 'detoxify_insult', 'detoxify_threat', 'detoxify_sexual_explicit']\n",
    "features_cols = ['pattern_sentiment', 'pattern_subjectivity', 'vader_sentiment', 'eng_spacysentiment', 'pattern_modality']\n",
    "\n",
    "if groupby_author:\n",
    "    features_df = pd.concat([train,test],axis=0,ignore_index=False).groupby('author').agg({c:'first' for c in train.columns if 'author_flair_text' in c}).join(features_df.drop(columns=['author_flair_text']))\n",
    "    for c in filter(lambda c: c.endswith('_bool'), features_df.columns):\n",
    "        features_df[c] = features_df[c].div(features_df['content_type'].map(lambda dct: sum(dct.values())))\n",
    "else:\n",
    "    features_df = pd.merge(pd.concat([train,test],axis=0,ignore_index=False).reset_index().convert_dtypes(),\n",
    "                    features_df.drop(columns=['author_flair_text','subreddit','text','clean_text'], errors='ignore').reset_index().convert_dtypes(), \n",
    "                    how='left', on=['content_type', 'content_id','author']).set_index(train.index.names)\n",
    "\n",
    "    \n",
    "train,test = features_df.loc[train.index], features_df.loc[test.index]\n",
    "del(features_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./data/models/ML/0/'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from data_extraction import get_whole_filelist\n",
    "models_dir = './data/models/ML/'\n",
    "curr_model_directory = \"{}{}/\".format(models_dir, \\\n",
    "                                       str (1 + max (\\\n",
    "                                           list(map(lambda x: int(x.replace('\\\\','/').split('/')[-1]), curr_models)) \\\n",
    "                                           if \n",
    "                                               len(curr_models:=[f for f in get_whole_filelist(models_dir) if os.path.isdir(f)]) \\\n",
    "                                           else \\\n",
    "                                               [-1]) )\\\n",
    ")\n",
    "curr_model_directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from author_flair_mapping import get_valid_authors, map_mother_categories\n",
    "\n",
    "load_from_disk = False\n",
    "if load_from_disk:\n",
    "    category_col = 'polcompass_flair'\n",
    "    train = pd.read_parquet(curr_model_directory+'train.parquet')\n",
    "    test = pd.read_parquet(curr_model_directory+'test.parquet')\n",
    "\n",
    "    vice_cols, virtue_cols =[c for c in train.columns if c.endswith('.vice')], [c for c in train.columns if c.endswith('.virtue')]\n",
    "    bias_cols = ['bias_loyalty', 'bias_care', 'bias_sanctity', 'bias_authority', 'bias_fairness']\n",
    "    intensity_cols = ['intensity_loyalty', 'intensity_care', 'intensity_sanctity', 'intensity_authority', 'intensity_fairness']\n",
    "\n",
    "    vad_cols = ['strong_valence_tokens_ratio', 'valence_mean', 'strong_arousal_tokens_ratio', 'arousal_mean', 'strong_dominance_tokens_ratio', 'dominance_mean']\n",
    "    nrclex_cols = list(pd.read_parquet('./data/dataset_whole/mapped_dataframes/comments_chunk_0_nrclex_new.parquet').columns.drop('num_tokens'))\n",
    "    social_cols = ['strong_socialness_tokens_ratio', 'socialness_mean']\n",
    "    detoxify_cols = ['detoxify_toxicity', 'detoxify_severe_toxicity', 'detoxify_obscene', 'detoxify_identity_attack', 'detoxify_insult', 'detoxify_threat', 'detoxify_sexual_explicit']\n",
    "    features_cols = ['pattern_sentiment', 'pattern_subjectivity', 'vader_sentiment', 'eng_spacysentiment', 'pattern_modality']\n",
    "    populism_cols = ['Populism', 'PeopleCentrism', 'AntiElitism', 'EmotionalAppeal']\n",
    "    \n",
    "    whole_count_cols = ['emoticons_count', 'emojis_count', 'nltk_entities_count', 'spacy_entities_count']\n",
    "    subcat_count_cols = [c for c in train.columns if c.endswith('_count') and c not in whole_count_cols+['sentences_count','tokens_count']]\n",
    "    subcat_count_cols = {c:c.split('_')[0]+'_entities_count' if c.startswith(('nltk_','spacy_')) else 'emoticons_count' if 'emoticons_count' in c else 'emojis_count' if 'emojis_count' in c else c for c in subcat_count_cols}\n",
    "\n",
    "\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "    #categories_df = map_mother_categories(train[[category_col]], category_col=category_col)\n",
    "    curr_dimension = None\n",
    "    global_label_encoder = LabelEncoder()\n",
    "    global_label_encoder.fit_transform(train[category_col].dropna())\n",
    "    global_label_mapping = {c:global_label_encoder.transform([c])[0] for c in global_label_encoder.classes_}\n",
    "\n",
    "    h_label_encoder = LabelEncoder()\n",
    "    h_label_encoder.fit_transform(train['horizontal_'+category_col].dropna())\n",
    "    h_label_mapping = {c:h_label_encoder.transform([c])[0] for c in h_label_encoder.classes_}\n",
    "\n",
    "    v_label_encoder = LabelEncoder()\n",
    "    v_label_encoder.fit_transform(train['vertical_'+category_col].dropna())\n",
    "    v_label_mapping = {c:v_label_encoder.transform([c])[0] for c in v_label_encoder.classes_}\n",
    "    \n",
    "    inv_global_label_mapping = {v:k for k,v in global_label_mapping.items()}\n",
    "    inv_h_label_mapping = {v:k for k,v in h_label_mapping.items()}\n",
    "    inv_v_label_mapping = {v:k for k,v in v_label_mapping.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "#from sklearn.model_selection import ParameterGrid\n",
    "from scipy.sparse import csr_matrix\n",
    "from scipy.sparse import hstack as scipy_hstack\n",
    "from scipy.sparse import vstack as scipy_vstack\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, MinMaxScaler\n",
    "\n",
    "#from sklearn.model_selection import ParameterGrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nsettings_dct = {\\'use_bool\\':use_bool, \\'use_generic_cats\\':use_generic_cats, \\'drop_tokens_and_sentences\\': drop_tokens_and_sentences,\\n               \\'use_minmax\\':use_minmax}\\nimport json\\n# Serialize data into file:\\njson.dump( settings_dct, open( curr_saving_directory+\"training_settings.json\", \\'w\\' ) )\\n\\n# Read data from file:\\nsettings_dct = json.load( open( curr_saving_directory+\"training_settings.json\" ) )\\n'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "use_bool=False\n",
    "use_generic_cats = False\n",
    "drop_tokens_and_sentences = False\n",
    "use_minmax = True\n",
    "\n",
    "\"\"\"\n",
    "settings_dct = {'use_bool':use_bool, 'use_generic_cats':use_generic_cats, 'drop_tokens_and_sentences': drop_tokens_and_sentences,\n",
    "               'use_minmax':use_minmax}\n",
    "import json\n",
    "# Serialize data into file:\n",
    "json.dump( settings_dct, open( curr_saving_directory+\"training_settings.json\", 'w' ) )\n",
    "\n",
    "# Read data from file:\n",
    "settings_dct = json.load( open( curr_saving_directory+\"training_settings.json\" ) )\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "if not (curr_path:=Path(curr_model_directory)).exists():\n",
    "    curr_path.mkdir(exist_ok=False, parents=True)\n",
    "train.to_parquet(curr_path.joinpath('train.parquet'))\n",
    "test.to_parquet(curr_path.joinpath('test.parquet'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXTRACTING BEST K TOKENS BY TFIDF FOR EACH DIMENSION/UNDERSAMPLING METHOD/BINARY CATEGORY AND STORING THEM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_parquet(curr_model_directory+'train.parquet')\n",
    "test = pd.read_parquet(curr_model_directory+'test.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'curr_dimension': 'horizontal', 'drop_centrists': False, 'drop_undefined': False}\n",
      "horizontal centrist tfidf20_horizontal_centrist\n",
      "horizontal_polcompass_flair_centrist\n",
      "False    304\n",
      "True      63\n",
      "Name: count, dtype: Int64\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\onest\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1108: UserWarning: One or more of the test scores are non-finite: [nan]\n",
      "  warnings.warn(\n",
      "C:\\Users\\onest\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "C:\\Users\\onest\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best ngram_range: (1, 1)\n",
      "horizontal left tfidf20_horizontal_left\n",
      "horizontal_polcompass_flair_left\n",
      "False    270\n",
      "True      97\n",
      "Name: count, dtype: Int64\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\onest\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1108: UserWarning: One or more of the test scores are non-finite: [nan]\n",
      "  warnings.warn(\n",
      "C:\\Users\\onest\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "C:\\Users\\onest\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best ngram_range: (1, 1)\n",
      "horizontal Undefined tfidf20_horizontal_Undefined\n",
      "horizontal_polcompass_flair_Undefined\n",
      "False    275\n",
      "True      92\n",
      "Name: count, dtype: Int64\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\onest\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1108: UserWarning: One or more of the test scores are non-finite: [nan]\n",
      "  warnings.warn(\n",
      "C:\\Users\\onest\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "C:\\Users\\onest\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best ngram_range: (1, 1)\n",
      "horizontal right tfidf20_horizontal_right\n",
      "horizontal_polcompass_flair_right\n",
      "False    252\n",
      "True     115\n",
      "Name: count, dtype: Int64\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\onest\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1108: UserWarning: One or more of the test scores are non-finite: [nan]\n",
      "  warnings.warn(\n",
      "C:\\Users\\onest\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "C:\\Users\\onest\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best ngram_range: (1, 1)\n",
      "horizontal whole tfidf20_horizontal_whole\n",
      "horizontal_polcompass_flair_whole\n",
      "right        115\n",
      "left          97\n",
      "Undefined     92\n",
      "centrist      63\n",
      "Name: count, dtype: Int64\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\onest\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1108: UserWarning: One or more of the test scores are non-finite: [nan]\n",
      "  warnings.warn(\n",
      "C:\\Users\\onest\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "C:\\Users\\onest\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best ngram_range: (1, 1)\n",
      "{'curr_dimension': 'horizontal', 'drop_centrists': False, 'drop_undefined': True}\n",
      "horizontal centrist tfidf20_horizontal_no_undefined_centrist\n",
      "horizontal_polcompass_flair_centrist\n",
      "False    212\n",
      "True      63\n",
      "Name: count, dtype: Int64\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\onest\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1108: UserWarning: One or more of the test scores are non-finite: [nan]\n",
      "  warnings.warn(\n",
      "C:\\Users\\onest\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "C:\\Users\\onest\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best ngram_range: (1, 1)\n",
      "horizontal left tfidf20_horizontal_no_undefined_left\n",
      "horizontal_polcompass_flair_left\n",
      "False    178\n",
      "True      97\n",
      "Name: count, dtype: Int64\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\onest\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1108: UserWarning: One or more of the test scores are non-finite: [nan]\n",
      "  warnings.warn(\n",
      "C:\\Users\\onest\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "C:\\Users\\onest\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best ngram_range: (1, 1)\n",
      "horizontal right tfidf20_horizontal_no_undefined_right\n",
      "horizontal_polcompass_flair_right\n",
      "False    160\n",
      "True     115\n",
      "Name: count, dtype: Int64\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\onest\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1108: UserWarning: One or more of the test scores are non-finite: [nan]\n",
      "  warnings.warn(\n",
      "C:\\Users\\onest\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "C:\\Users\\onest\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best ngram_range: (1, 1)\n",
      "horizontal whole tfidf20_horizontal_no_undefined_whole\n",
      "horizontal_polcompass_flair_whole\n",
      "right       115\n",
      "left         97\n",
      "centrist     63\n",
      "Name: count, dtype: Int64\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\onest\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1108: UserWarning: One or more of the test scores are non-finite: [nan]\n",
      "  warnings.warn(\n",
      "C:\\Users\\onest\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "C:\\Users\\onest\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best ngram_range: (1, 1)\n",
      "{'curr_dimension': 'horizontal', 'drop_centrists': True, 'drop_undefined': False}\n",
      "horizontal left tfidf20_horizontal_no_centrist_left\n",
      "horizontal_polcompass_flair_left\n",
      "False    207\n",
      "True      97\n",
      "Name: count, dtype: Int64\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\onest\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1108: UserWarning: One or more of the test scores are non-finite: [nan]\n",
      "  warnings.warn(\n",
      "C:\\Users\\onest\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "C:\\Users\\onest\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "C:\\Users\\onest\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1108: UserWarning: One or more of the test scores are non-finite: [nan]\n",
      "  warnings.warn(\n",
      "C:\\Users\\onest\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best ngram_range: (1, 1)\n",
      "horizontal Undefined tfidf20_horizontal_no_centrist_Undefined\n",
      "horizontal_polcompass_flair_Undefined\n",
      "False    212\n",
      "True      92\n",
      "Name: count, dtype: Int64\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Best ngram_range: (1, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\onest\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "C:\\Users\\onest\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1108: UserWarning: One or more of the test scores are non-finite: [nan]\n",
      "  warnings.warn(\n",
      "C:\\Users\\onest\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "C:\\Users\\onest\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "horizontal right tfidf20_horizontal_no_centrist_right\n",
      "horizontal_polcompass_flair_right\n",
      "False    189\n",
      "True     115\n",
      "Name: count, dtype: Int64\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Best ngram_range: (1, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import itertools, pathlib, os\n",
    "from sklearn.base import clone as sklearn_clone\n",
    "from utils import flatten\n",
    "from tfidf_utils import do_nothing, Chi2ScorerWrapper\n",
    "\n",
    "pathlib.Path(curr_model_directory+'/tfidf/train/').mkdir(parents=True, exist_ok=True)\n",
    "pathlib.Path(curr_model_directory+'/tfidf/test/').mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "ngram_range = (1,1)\n",
    "tfidf_pipeline = Pipeline([\n",
    "    ('vectorizer', CountVectorizer(tokenizer=do_nothing, preprocessor=do_nothing, ngram_range=ngram_range)),  # frequencies\n",
    "    ('tfidf', TfidfTransformer()),  # tfidf\n",
    "    ('kbest', Chi2ScorerWrapper(score_func=chi2, k=20)),\n",
    "])\n",
    "\n",
    "param_grid_tfidf = {\n",
    "    'vectorizer__ngram_range': [(1, 1)],     # unigrams, bigrams, trigrams\n",
    "}\n",
    "models_params_dct = {'curr_dimension': ['global', 'horizontal', 'vertical'],\n",
    "       'drop_centrists':[False, True],\n",
    "       'drop_undefined':[False, True],\n",
    "}\n",
    "models_params_dct = {k:list(set(v)) for k,v in models_params_dct.items()}\n",
    "\n",
    "text_col, category_col  = 'filtered_tokens', 'polcompass_flair'\n",
    "\n",
    "top_words_by_cat = {}\n",
    "for tup in itertools.product(*models_params_dct.values()):\n",
    "\n",
    "    curr_params = {list(models_params_dct.keys())[i]:tup[i] for i in range(len(tup))}\n",
    "    print(curr_params)\n",
    "    locals().update(curr_params)\n",
    "    \n",
    "    curr_cat_col = category_col if curr_dimension=='global' else curr_dimension+'_'+category_col\n",
    "\n",
    "    curr_train = train.loc[train[curr_cat_col]==train['{}Dimension'.format(curr_dimension.capitalize())]]\\\n",
    "            if curr_dimension in ['vertical','horizontal'] \\\n",
    "            else \\\n",
    "                train.loc[(train['VerticalDimension'].fillna('centrist')==train['vertical_polcompass_flair'].fillna('centrist')) & (train['HorizontalDimension'].fillna('centrist')==train['horizontal_polcompass_flair'].fillna('centrist'))]\n",
    "    if drop_centrists:\n",
    "        curr_train = curr_train.loc[curr_train[curr_cat_col]!='centrist']\n",
    "        if curr_dimension not in ['horizontal', 'vertical']:\n",
    "            curr_train = curr_train[(curr_train['horizontal_polcompass_flair']!='centrist') & (curr_train['vertical_polcompass_flair']!='centrist')]\n",
    "    if drop_undefined:\n",
    "        curr_train = curr_train.loc[curr_train[curr_cat_col]!='Undefined']\n",
    "    curr_train = get_curr_dataset(curr_train, training_col=text_col, category_col=category_col, dimension=curr_dimension, dropna=True)\n",
    "\n",
    "\n",
    "    curr_test = test.loc[test[curr_cat_col]==test['{}Dimension'.format(curr_dimension.capitalize())]] \\\n",
    "            if curr_dimension in ['vertical','horizontal'] \\\n",
    "            else \\\n",
    "                test.loc[(test['VerticalDimension'].fillna('centrist')==test['vertical_polcompass_flair'].fillna('centrist')) & (test['HorizontalDimension'].fillna('centrist')==test['horizontal_polcompass_flair'].fillna('centrist'))]\n",
    "    if drop_centrists:\n",
    "        curr_test = curr_test.loc[curr_test[curr_cat_col]!='centrist']\n",
    "        if curr_dimension not in ['horizontal', 'vertical']:\n",
    "            curr_test = curr_test[(curr_test['horizontal_polcompass_flair']!='centrist') & (curr_test['vertical_polcompass_flair']!='centrist')]\n",
    "    if drop_undefined:\n",
    "        curr_test = curr_test.loc[curr_test[curr_cat_col]!='Undefined']\n",
    "    curr_test = get_curr_dataset(curr_test, training_col=text_col, category_col=category_col, dimension=curr_dimension, dropna=True)\n",
    "    x_train = curr_train[text_col].values\n",
    "    for single_category in np.append(curr_train.iloc[:, -1].unique(), ['whole']):\n",
    "\n",
    "        tfidf_col = 'tfidf20_{}{}_{}'.format(curr_dimension, '_no_centrist_no_undefined' if drop_centrists and drop_undefined else '_no_centrist' if drop_centrists else '_no_undefined' if drop_undefined else '', single_category)\n",
    "        print(curr_dimension, single_category, tfidf_col)\n",
    "        #unique_y_col = train[curr_y_col].dropna().unique()\n",
    "        #curr_train[class_name] = (curr_train.iloc[:,-1]=='lib').astype('float')\n",
    "        #for unique_y in unique_y_col:\n",
    "        y_train = curr_train.iloc[:,-1] if single_category=='whole' else (curr_train.iloc[:,-1]==single_category)\n",
    "        y_train.name = '_'.join([y_train.name, single_category])\n",
    "\n",
    "        tfidf_k20 = sklearn_clone(tfidf_pipeline)\n",
    "        tfidf_k20.set_params(**{'kbest__k':20})\n",
    "        # Best NGrams through 5-Fold Cross Validation\n",
    "        grid = GridSearchCV(tfidf_k20, param_grid_tfidf, cv=5, scoring=None, n_jobs=-1, verbose=1)\n",
    "        print(y_train.value_counts())\n",
    "        grid.fit(x_train, y_train)\n",
    "        print(\"Best ngram_range:\", grid.best_params_['vectorizer__ngram_range'])\n",
    "\n",
    "        tfidf20 = grid.best_estimator_.fit_transform(x_train, y_train)\n",
    "        train_tfidf = pd.Series(tfidf20.toarray().tolist(), index=curr_train.index, name=tfidf_col)\n",
    "        train_tfidf.to_pickle(os.path.join(curr_model_directory, 'tfidf/train/{}.pkl'.format(tfidf_col)) )\n",
    "\n",
    "        x_test, y_test = curr_test[text_col].values, curr_test.iloc[:,-1]\n",
    "        tfidf20_test = grid.best_estimator_.transform(x_test)\n",
    "        test_tfidf = pd.Series(tfidf20_test.toarray().tolist(), index=curr_test.index, name=tfidf_col)\n",
    "        test_tfidf.to_pickle(os.path.join(curr_model_directory, 'tfidf/test/{}.pkl'.format(tfidf_col)) )\n",
    "\n",
    "        best_estim = grid.best_estimator_\n",
    "        ngrams = best_estim.named_steps['vectorizer'].get_feature_names_out()\n",
    "        best_ngrams = ngrams[best_estim.named_steps['kbest'].selector_.get_support()]\n",
    "        top_words_by_cat[y_train.name] = best_ngrams\n",
    "        del(tfidf_k20, tfidf20, tfidf20_test, train_tfidf, test_tfidf)\n",
    "\n",
    "#pd.to_pickle(obj=top_words_by_cat, filepath_or_buffer=curr_model_directory+'/tfidf_best_{}.pickle'.format('words' if ngram_range[0]==ngram_range[1] and ngram_range[0]==1 else 'ngrams') )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"for k in sorted(top_words_by_cat, key=lambda x: :\\n    print(k, top_words_by_cat.get(k), '\\n')\""
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import functools\n",
    "\n",
    "def cust_sort_tfidf(a,b):\n",
    "    def cust_sort_dim():\n",
    "        return {'horizontal':2, 'vertical':1, 'polcompass':0}\n",
    "    dimension_a, dimension_b = a.split('_')[0], b.split('_')[0]\n",
    "    #print(dimension_a, dimension_b)\n",
    "    if dimension_a == dimension_b:\n",
    "        #print('same dim..', a, b, '... is {} greater than {}?'.format(a,b), a>b)\n",
    "        return 1 if a>=b else -1\n",
    "    else:\n",
    "        sorted_dimensions = cust_sort_dim()\n",
    "        return 2 * int(sorted_dimensions.get(dimension_a, 3)<sorted_dimensions.get(dimension_b, 3)) - 1\n",
    "\"\"\"for k in sorted(top_words_by_cat, key=lambda x: :\n",
    "    print(k, top_words_by_cat.get(k), '\\n')\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "horizontal_polcompass_flair_Undefined ['aids' 'angry' 'biafra' 'confirmed' 'covid' 'flex' 'genocide' 'grows'\n",
      " 'guts' 'guy' 'hitler' 'holy' 'mtg' 'pedo-defenders' 'prisoners' 'putin'\n",
      " 'socialism' 'speaker' 'unpersoned' 'vaxxed'] \n",
      "\n",
      "\n",
      "horizontal_polcompass_flair_centrist ['anti-capitalism' 'attention' 'collectivist' 'cops' 'democracys'\n",
      " 'disease' 'eu' 'flags' 'georgism' 'ideologies' 'individualist'\n",
      " 'intervene' 'investigation' 'kind' 'nhs' 'obama' 'politics' 'twitter'\n",
      " 'vaccinated' 'voters'] \n",
      "\n",
      "\n",
      "horizontal_polcompass_flair_left ['bernie' 'capitalist' 'communist' 'democracy' 'democratic' 'fuck' 'got'\n",
      " 'help' 'lol' 'marx' 'mutualist' 'political' 'power' 'revolution' 'social'\n",
      " 'socialism' 'socialist' 'states' \"there's\" 'united'] \n",
      "\n",
      "\n",
      "horizontal_polcompass_flair_right ['bernie' 'capitalist' 'communist' 'democracy' 'democratic' 'fuck' 'got'\n",
      " 'help' 'lol' 'marx' 'mutualist' 'political' 'power' 'revolution' 'social'\n",
      " 'socialism' 'socialist' 'states' \"there's\" 'united'] \n",
      "\n",
      "\n",
      "horizontal_polcompass_flair_whole ['bernie' 'capitalist' 'communist' 'democracy' 'democratic' 'fuck' 'got'\n",
      " 'help' 'lol' 'marx' 'mutualist' 'political' 'power' 'revolution' 'social'\n",
      " 'socialism' 'socialist' 'states' \"there's\" 'united'] \n",
      "\n",
      "\n",
      "vertical_polcompass_flair_Undefined ['aids' 'believe' 'biafra' 'bro' 'confirmed' 'covid' 'disgusting' 'free'\n",
      " 'funny' 'gives' 'israel' 'jobs' 'libertarian' 'libertarians' 'lol'\n",
      " 'right' 'russia' 'share' 'vaxxed' 'wrong'] \n",
      "\n",
      "\n",
      "vertical_polcompass_flair_auth ['angry' 'asshole' 'camp' 'china' 'concentration' 'deport' 'fellow'\n",
      " 'france' 'jewish' 'lenin' 'monarchism' 'monarcho' 'monarchy' 'nazis'\n",
      " 'packs' 'pede' 'proved' 'stalin' 'thank' 'vive'] \n",
      "\n",
      "\n",
      "vertical_polcompass_flair_centrist ['afford' 'akkad' 'bill' 'businessman' 'dan' 'deutschland' 'eu' 'girls'\n",
      " 'healthcare' 'implemented' 'nhs' 'obama' 'page' 'probably' 'reverses'\n",
      " 'sargon' 'surprised' 'trump' 'universal' 'v'] \n",
      "\n",
      "\n",
      "vertical_polcompass_flair_lib ['angry' 'asshole' 'camp' 'china' 'concentration' 'deport' 'fellow'\n",
      " 'france' 'jewish' 'lenin' 'monarchism' 'monarcho' 'monarchy' 'nazis'\n",
      " 'packs' 'pede' 'proved' 'stalin' 'thank' 'vive'] \n",
      "\n",
      "\n",
      "vertical_polcompass_flair_whole ['angry' 'asshole' 'camp' 'china' 'concentration' 'deport' 'fellow'\n",
      " 'france' 'jewish' 'lenin' 'monarchism' 'monarcho' 'monarchy' 'nazis'\n",
      " 'packs' 'pede' 'proved' 'stalin' 'thank' 'vive'] \n",
      "\n",
      "\n",
      "polcompass_flair_Undefined ['aids' 'angry' 'based' 'bro' 'community' 'france' 'free' 'give'\n",
      " 'government' 'israel' 'libertarian' 'market' 'mean' 'monarchy' 'putin'\n",
      " 'return' 'revolution' 'socialism' 'though' 'wrong'] \n",
      "\n",
      "\n",
      "polcompass_flair_authleft ['class' 'communist' 'democratic' 'growing' 'lenin' 'marx' 'nations' 'new'\n",
      " 'parties' 'prior' 'proved' 'realize' 'republic' 'rule' 'slandering'\n",
      " 'socialism' 'stalin' 'succeeded' 'united' 'vaushites'] \n",
      "\n",
      "\n",
      "polcompass_flair_authright ['asshole' 'back' 'conservative' 'deport' 'fellow' 'fitting' 'france'\n",
      " 'king' 'louis' 'monarchism' 'monarchy' 'natcon' 'national' 'pede'\n",
      " 'religious' 'revival' 'said' 'thank' 'vive' 'xx'] \n",
      "\n",
      "\n",
      "polcompass_flair_centrist ['areas' 'bill' 'conceivably' 'cyprus' 'dems' 'due' 'eu' 'extradited'\n",
      " 'forces' 'healthcare' 'implemented' 'nhs' 'obama' 'producers' 'reverses'\n",
      " 'sound' 'surprised' 'threatening' 'twitter' 'universal'] \n",
      "\n",
      "\n",
      "polcompass_flair_left ['campaign' 'centrists' 'cons' 'epic' 'explicitly' 'exploit' 'gop'\n",
      " 'happening' 'interference' 'intervention' 'liberalism' 'machines'\n",
      " 'opinion' 'posts' 'progress' 'smaller' 'turned' 'violent' \"what's\"\n",
      " 'worker'] \n",
      "\n",
      "\n",
      "polcompass_flair_libcenter ['ancaps' 'centrist' 'choice' 'clauses' 'crafts' 'cultural' 'facebook'\n",
      " 'identity' 'incredibly' 'language' 'policy' 'radical' 'remember'\n",
      " 'restriction' 'socialists' 'speech' 'sway' 'tons' 'vaccinated' 'view'] \n",
      "\n",
      "\n",
      "polcompass_flair_libleft ['anarchist' 'beat' 'cnt-fai' 'communes' 'community' 'cracker' 'enforced'\n",
      " 'freestate' 'ftw' 'help' 'lib' 'pkk' 'rail' 'respond' 'revolution' 'shut'\n",
      " 'supposed' 'syrian' 'viva' 'yay'] \n",
      "\n",
      "\n",
      "polcompass_flair_libright ['anything' 'believe' 'employees' 'france' 'government' 'guns' 'idgaf'\n",
      " 'keeping' 'liberland' 'libertarianism' 'monarchy' 'power' 'pro-gun'\n",
      " 'pro-politician' 'singapore' 'socialism' 'sport' 'us' 'wants' 'welfare'] \n",
      "\n",
      "\n",
      "polcompass_flair_right ['antigay' 'blm' 'chairman' 'changed' 'clinton' 'convicted' 'corrected'\n",
      " 'feed' 'funded' 'leave' 'losing' 'politicians' 'republican' 'republicans'\n",
      " 'stereotype' 'super' 'themself' 'tumor' 'types' 'waved'] \n",
      "\n",
      "\n",
      "polcompass_flair_whole ['class' 'democratic' 'deport' 'fellow' 'france' 'lenin' 'marx'\n",
      " 'monarchism' 'monarchy' 'nations' 'new' 'parties' 'pede' 'proved'\n",
      " 'socialism' 'stalin' 'thank' 'united' 'vive' 'wrong'] \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for k in sorted(top_words_by_cat, key=functools.cmp_to_key(cust_sort_tfidf)):\n",
    "    print(k, top_words_by_cat[k], '\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TRAINING MODELS (CV, FOR ALL PARAMETERS)\n",
    "##### For each DIMENSION = ['global', 'horizontal', 'vertical']; UNDERSAMPLING = ['smote', 'min', False]; DROP_UNDEFINED=[True, False]; DROP_CENTRIST=[True, False]... Training model on each possible combination of these parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_model_directory #= './data/models/ML/final/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No estimators previously saved\n",
      "{'curr_dimension': 'global', 'undersample': False, 'drop_undefined': True, 'drop_centrists': False}\n",
      "polcompass_flair\n",
      "libright     36\n",
      "libleft      26\n",
      "centrist     19\n",
      "authright    12\n",
      "left         12\n",
      "right         9\n",
      "libcenter     8\n",
      "authleft      6\n",
      "Name: count, dtype: Int64\n",
      "LinearSVC(max_iter=3000) [{'C': [0.1, 1, 10], 'class_weight': ['balanced'], 'loss': ['hinge', 'squared_hinge'], 'penalty': ['l2']}, {'C': [0.1, 1, 10], 'class_weight': ['balanced'], 'loss': ['squared_hinge'], 'penalty': ['l1']}]\n",
      "Fitting 5 folds for each of 9 candidates, totalling 45 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\onest\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\svm\\_base.py:1249: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    authleft       0.00      0.00      0.00         1\n",
      "   authright       1.00      0.33      0.50         6\n",
      "    centrist       0.33      0.17      0.22         6\n",
      "        left       0.00      0.00      0.00         4\n",
      "     libleft       0.14      0.33      0.20         3\n",
      "    libright       0.60      0.60      0.60        10\n",
      "       right       0.33      0.50      0.40         4\n",
      "\n",
      "    accuracy                           0.35        34\n",
      "   macro avg       0.34      0.28      0.27        34\n",
      "weighted avg       0.46      0.35      0.37        34\n",
      "\n",
      "Confusion matrix:\n",
      "[[0 0 0 0 1 0 0]\n",
      " [0 2 1 0 1 1 1]\n",
      " [0 0 1 2 2 0 1]\n",
      " [0 0 0 0 2 1 1]\n",
      " [1 0 0 0 1 1 0]\n",
      " [0 0 1 2 0 6 1]\n",
      " [0 0 0 1 0 1 2]]\n",
      "RandomForestClassifier() [{'criterion': ['gini', 'entropy'], 'n_estimators': [10, 100], 'min_samples_split': [0.001, 0.02], 'class_weight': ['balanced']}]\n",
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    authleft       0.00      0.00      0.00         1\n",
      "   authright       0.00      0.00      0.00         6\n",
      "    centrist       0.33      0.33      0.33         6\n",
      "        left       0.00      0.00      0.00         4\n",
      "   libcenter       0.00      0.00      0.00         0\n",
      "     libleft       0.00      0.00      0.00         3\n",
      "    libright       0.48      1.00      0.65        10\n",
      "       right       0.20      0.25      0.22         4\n",
      "\n",
      "    accuracy                           0.38        34\n",
      "   macro avg       0.13      0.20      0.15        34\n",
      "weighted avg       0.22      0.38      0.27        34\n",
      "\n",
      "Confusion matrix:\n",
      "[[ 0  0  0  0  0  0  0  1]\n",
      " [ 0  0  3  0  0  0  1  2]\n",
      " [ 0  0  2  0  0  1  3  0]\n",
      " [ 0  0  0  0  0  0  3  1]\n",
      " [ 0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  1  0  2  0]\n",
      " [ 0  0  0  0  0  0 10  0]\n",
      " [ 0  0  1  0  0  0  2  1]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\onest\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\onest\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\onest\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\onest\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\onest\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\onest\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Shape of passed values is (8, 8), indices imply (7, 7)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[41]\u001b[39m\u001b[32m, line 177\u001b[39m\n\u001b[32m    175\u001b[39m \u001b[38;5;28mprint\u001b[39m(cm)\n\u001b[32m    176\u001b[39m labels = \u001b[38;5;28msorted\u001b[39m(y_test.unique())\n\u001b[32m--> \u001b[39m\u001b[32m177\u001b[39m cm_df = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    178\u001b[39m cm_df.to_parquet(curr_saving_directory+\u001b[33m'\u001b[39m\u001b[33mcm__best_\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m.parquet\u001b[39m\u001b[33m'\u001b[39m.format (models_names_dict[\u001b[38;5;28mstr\u001b[39m(estimator).split(\u001b[33m'\u001b[39m\u001b[33m(\u001b[39m\u001b[33m'\u001b[39m)[\u001b[32m0\u001b[39m]]) )\n\u001b[32m    180\u001b[39m pd.to_pickle(obj=whole_clf_gridsearch, filepath_or_buffer=curr_saving_directory+\u001b[33m'\u001b[39m\u001b[33mgrid_search_\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m.pkl\u001b[39m\u001b[33m'\u001b[39m.format(models_names_dict[\u001b[38;5;28mstr\u001b[39m(estimator).split(\u001b[33m'\u001b[39m\u001b[33m(\u001b[39m\u001b[33m'\u001b[39m)[\u001b[32m0\u001b[39m]]))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\frame.py:827\u001b[39m, in \u001b[36mDataFrame.__init__\u001b[39m\u001b[34m(self, data, index, columns, dtype, copy)\u001b[39m\n\u001b[32m    816\u001b[39m         mgr = dict_to_mgr(\n\u001b[32m    817\u001b[39m             \u001b[38;5;66;03m# error: Item \"ndarray\" of \"Union[ndarray, Series, Index]\" has no\u001b[39;00m\n\u001b[32m    818\u001b[39m             \u001b[38;5;66;03m# attribute \"name\"\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    824\u001b[39m             copy=_copy,\n\u001b[32m    825\u001b[39m         )\n\u001b[32m    826\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m827\u001b[39m         mgr = \u001b[43mndarray_to_mgr\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    828\u001b[39m \u001b[43m            \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    829\u001b[39m \u001b[43m            \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    830\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    831\u001b[39m \u001b[43m            \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    832\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    833\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtyp\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmanager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    834\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    836\u001b[39m \u001b[38;5;66;03m# For data is list-like, or Iterable (will consume into list)\u001b[39;00m\n\u001b[32m    837\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m is_list_like(data):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\internals\\construction.py:336\u001b[39m, in \u001b[36mndarray_to_mgr\u001b[39m\u001b[34m(values, index, columns, dtype, copy, typ)\u001b[39m\n\u001b[32m    331\u001b[39m \u001b[38;5;66;03m# _prep_ndarraylike ensures that values.ndim == 2 at this point\u001b[39;00m\n\u001b[32m    332\u001b[39m index, columns = _get_axes(\n\u001b[32m    333\u001b[39m     values.shape[\u001b[32m0\u001b[39m], values.shape[\u001b[32m1\u001b[39m], index=index, columns=columns\n\u001b[32m    334\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m336\u001b[39m \u001b[43m_check_values_indices_shape_match\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    338\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m typ == \u001b[33m\"\u001b[39m\u001b[33marray\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    339\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(values.dtype.type, \u001b[38;5;28mstr\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\internals\\construction.py:420\u001b[39m, in \u001b[36m_check_values_indices_shape_match\u001b[39m\u001b[34m(values, index, columns)\u001b[39m\n\u001b[32m    418\u001b[39m passed = values.shape\n\u001b[32m    419\u001b[39m implied = (\u001b[38;5;28mlen\u001b[39m(index), \u001b[38;5;28mlen\u001b[39m(columns))\n\u001b[32m--> \u001b[39m\u001b[32m420\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mShape of passed values is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpassed\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, indices imply \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimplied\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mValueError\u001b[39m: Shape of passed values is (8, 8), indices imply (7, 7)"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from scipy.sparse import csr_matrix\n",
    "from scipy.sparse import hstack as scipy_hstack\n",
    "from scipy.sparse import vstack as scipy_vstack\n",
    "import time, os, pickle\n",
    "from pathlib import Path\n",
    "from utils import flatten\n",
    "import itertools\n",
    "\n",
    "\n",
    "drop_nltk_entities = True\n",
    "models_params_dct = {'curr_dimension': ['global', 'vertical', 'horizontal'],\n",
    "                     'undersample':[False, 'min', 'smote'],\n",
    "                     'drop_undefined':[True, False],\n",
    "                     'drop_centrists':[False, True],\n",
    "}\n",
    "#models_params_dct = {k:list(set(v)) for k,v in models_params_dct.items()}\n",
    "models_names_dict = {'LinearSVC': 'svc', 'RandomForestClassifier': 'rf', 'LogisticRegression': 'lr'}\n",
    "\n",
    "estimators_filename = 'estimators.pkl'\n",
    "try:\n",
    "    estimators = pd.read_pickle(curr_model_directory+estimators_filename)\n",
    "    print(estimators.shape)\n",
    "except:\n",
    "    estimators = None\n",
    "    print('No estimators previously saved')\n",
    "\n",
    "\n",
    "populism_cols = ['Populism','PeopleCentrism','AntiElitism','EmotionalAppeal']\n",
    "curr_cols = pd.Index(set(features_cols+nrclex_cols+vice_cols+virtue_cols+bias_cols+intensity_cols+vad_cols+social_cols+detoxify_cols+populism_cols+ ['tokens_count','sentences_count'] + list(set(subcat_count_cols.values())) + list(subcat_count_cols.keys())) )\n",
    "if drop_nltk_entities:\n",
    "    curr_cols = curr_cols.drop(['nltk_organization_count', 'nltk_gsp_count', 'nltk_gpe_count', 'nltk_entities_count',\n",
    " 'nltk_location_count', 'nltk_facility_count', 'nltk_person_count'], errors='ignore')\n",
    "#std_scaler = StandardScaler()\n",
    "min_max_scaler = MinMaxScaler()\n",
    "for tup in itertools.product(*models_params_dct.values()):\n",
    "    curr_params = {list(models_params_dct.keys())[i]:tup[i] for i in range(len(tup))}\n",
    "    print(curr_params)\n",
    "    locals().update(curr_params)\n",
    "\n",
    "    curr_cat_col = category_col if curr_dimension=='global' else curr_dimension+'_'+category_col\n",
    "    tfidf_col = 'tfidf20_{}{}_whole'.format(curr_dimension, '_no_centrist_no_undefined' if drop_centrists and drop_undefined else '_no_centrist' if drop_centrists else '_no_undefined' if drop_undefined else '')\n",
    "\n",
    "    curr_saving_directory = curr_model_directory+'{}/{}{}'.format(curr_dimension, 'no_centrist_no_undefined/' if drop_centrists and drop_undefined else 'no_centrist/' if drop_centrists else 'no_undefined/' if drop_undefined else 'whole/', 'no_undersampling/' if not undersample else 'undersampled_max/' if undersample.lower()=='max' else 'smote/' if undersample.lower()=='smote' else 'undersampled_min/')\n",
    "    output_dir = Path(curr_saving_directory)\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    already_trained = {model_name: 'cm__best_{}.parquet'.format(model_name) in os.listdir(curr_saving_directory) for model_name in  models_names_dict.values()}\n",
    "    if os.path.exists(curr_saving_directory) and all(already_trained.values()):\n",
    "        continue\n",
    "\n",
    "    train = pd.read_parquet(curr_model_directory+'train.parquet')\n",
    "    tfidf_train = pd.read_pickle(os.path.join(curr_model_directory, 'tfidf/train/{}.pkl'.format(tfidf_col))).rename(tfidf_col)\n",
    "    tfidf_train = pd.DataFrame(tfidf_train.dropna().tolist(), index=tfidf_train.dropna().index).rename(lambda x: 'word_feat_'+str(x), axis=1)\n",
    "    \n",
    "    if any(already_trained.values()):\n",
    "      min_max_scaler=pd.read_pickle(curr_saving_directory+'minmax_scaler.pkl')\n",
    "      with open(curr_saving_directory+'training_cols.txt', 'r') as f:\n",
    "        training_cols = f.read().splitlines()\n",
    "        \n",
    "      if undersample=='smote':\n",
    "            train = pd.read_parquet(curr_saving_directory+'train_smote.parquet')\n",
    "      else:\n",
    "        train_index = pd.read_pickle(curr_saving_directory+'train_index.pkl')\n",
    "          \n",
    "        if sorted(train_index.join(train.index, how='inner')) != sorted(train_index):\n",
    "          raise ValueError('Wrong index for the current training set!!!')\n",
    "        train = train.loc[train_index].join(tfidf_train, how='left').loc[:,training_cols+[curr_cat_col]]\n",
    "    else:\n",
    "        train = train.loc[train[curr_cat_col]==train['{}Dimension'.format(curr_dimension.capitalize())]]\\\n",
    "                if curr_dimension in ['vertical','horizontal'] \\\n",
    "                else \\\n",
    "                    train.loc[(train['VerticalDimension'].fillna('centrist')==train['vertical_polcompass_flair'].fillna('centrist')) & (train['HorizontalDimension'].fillna('centrist')==train['horizontal_polcompass_flair'].fillna('centrist'))]\n",
    "        if drop_centrists:\n",
    "            train = train.loc[train[curr_cat_col]!='centrist']\n",
    "            if curr_dimension not in ['horizontal', 'vertical']:\n",
    "              train = train[(train['horizontal_polcompass_flair']!='centrist') & (train['vertical_polcompass_flair']!='centrist')]\n",
    "        if drop_undefined:\n",
    "            train = train.loc[train[curr_cat_col]!='Undefined']\n",
    "\n",
    "        try:\n",
    "          if sorted(tfidf_train.index) != sorted(train.index):\n",
    "            raise ValueError('Incorrect tf-idf index')\n",
    "        except ValueError:\n",
    "          tfidf_train = pd.read_pickle(os.path.join(curr_model_directory, 'tfidftmp/train/{}.pkl'.format(tfidf_col+'_extreme'))).rename(tfidf_col)\n",
    "          tfidf_train = pd.DataFrame(tfidf_train.dropna().tolist(), index=tfidf_train.dropna().index).rename(lambda x: 'word_feat_'+str(x), axis=1)\n",
    "          if sorted(tfidf_train.index) != sorted(train.index):\n",
    "            raise ValueError('Incorrect tf-idf index')\n",
    "\n",
    "        train = train.join(tfidf_train, how='left')\n",
    "\n",
    "        train = get_curr_dataset(train, category_col=category_col, dimension=curr_dimension, dropna=True, undersample=undersample,\n",
    "                                      training_col=[c for c in train.columns if c.startswith('word_feat_')]+list(curr_cols.drop([c for c in train.columns if category_col in c or 'tfidf' in c], errors='ignore')))\n",
    "    x_train = train.iloc[:, :-1]\n",
    "    x_train.columns = x_train.columns.astype(str)\n",
    "\n",
    "    x_train = pd.DataFrame(min_max_scaler.fit_transform(x_train), columns=x_train.columns, index=x_train.index) #x_train.apply(lambda x: (x-x.min())/(x.max()-x.min()) if x.min()<0 or x.max()>1 else x) ###min-max normalization\n",
    "    y_train = train.iloc[:,-1]   ##category col\n",
    "    print(y_train.value_counts(dropna=False))\n",
    "    y_encoded = eval('{}_label_encoder'.format(curr_dimension[0] if curr_dimension!='global' else curr_dimension)).transform(y_train)\n",
    "\n",
    "    training_cols = [c for c in x_train.columns if not c.isnumeric()] + [c for c in train.columns if 'tfidf' in c]\n",
    "    if undersample and undersample.lower()=='smote' and not any(already_trained.values()):\n",
    "        train.to_parquet(curr_saving_directory+'train_smote.parquet')\n",
    "    del(train, tfidf_train)\n",
    "\n",
    "    test = pd.read_parquet(curr_model_directory+'test.parquet')\n",
    "    tfidf_test = pd.read_pickle(os.path.join(curr_model_directory, 'tfidf/test/{}.pkl'.format(tfidf_col))).rename(tfidf_col)\n",
    "    tfidf_test = pd.DataFrame(tfidf_test.dropna().tolist(), index=tfidf_test.dropna().index).rename(lambda x: 'word_feat_'+str(x), axis=1)\n",
    "    if any(already_trained.values()):\n",
    "        test_index = pd.read_pickle(curr_saving_directory+'test_index.pkl')\n",
    "        with open(curr_saving_directory+'training_cols.txt', 'r') as f:\n",
    "            training_cols = f.read().splitlines()\n",
    "        if sorted(test_index.join(test.index, how='inner')) != sorted(test_index):\n",
    "            raise ValueError('Wrong index for the current test set!!!')\n",
    "        test = test.loc[test_index].join(tfidf_test, how='left').loc[:,training_cols+[curr_cat_col]]\n",
    "        min_max_scaler=pd.read_pickle(curr_saving_directory+'minmax_scaler.pkl')\n",
    "    else:\n",
    "        test = test.loc[test['_'.join([curr_dimension, category_col])]==test['{}Dimension'.format(curr_dimension.capitalize())]] \\\n",
    "                if curr_dimension in ['vertical','horizontal'] \\\n",
    "                else \\\n",
    "                    test.loc[(test['VerticalDimension'].fillna('centrist')==test['vertical_polcompass_flair'].fillna('centrist')) & (test['HorizontalDimension'].fillna('centrist')==test['horizontal_polcompass_flair'].fillna('centrist'))]\n",
    "        if drop_centrists:\n",
    "            test = test.loc[test[curr_cat_col]!='centrist']\n",
    "            if curr_dimension not in ['horizontal', 'vertical']:\n",
    "              test = test[(test['horizontal_polcompass_flair']!='centrist') & (test['vertical_polcompass_flair']!='centrist')]\n",
    "        if drop_undefined:\n",
    "            test = test.loc[test[curr_cat_col]!='Undefined']\n",
    "        try:\n",
    "          if sorted(tfidf_test.index) != sorted(test.index):\n",
    "            raise ValueError('Incorrect tf-idf index')\n",
    "        except ValueError:\n",
    "          tfidf_test = pd.read_pickle(os.path.join(curr_model_directory, 'tfidftmp/test/{}.pkl'.format(tfidf_col+'_extreme'))).rename(tfidf_col)\n",
    "          tfidf_test = pd.DataFrame(tfidf_test.dropna().tolist(), index=tfidf_test.dropna().index).rename(lambda x: 'word_feat_'+str(x), axis=1)\n",
    "          if sorted(tfidf_test.index) != sorted(test.index):\n",
    "            raise ValueError('Incorrect tf-idf index')\n",
    "        test = test.join(tfidf_test, how='left')\n",
    "\n",
    "        test = get_curr_dataset(test, category_col=category_col, dimension=curr_dimension, dropna=True, undersample=False,\n",
    "                                      training_col=list(set(test.columns) & set(x_train.columns)) )\n",
    "\n",
    "    x_test = test.loc[:, x_train.columns].rename(lambda x: str(x), axis=1).reindex(x_train.columns, axis=1)\n",
    "    x_test = pd.DataFrame(min_max_scaler.transform(x_test), columns=x_test.columns, index=x_test.index)\n",
    "    y_test = test.loc[:, y_train.name]   ##category col\n",
    "    del(test, tfidf_test)\n",
    "    if not any(already_trained.values()):\n",
    "        pd.to_pickle(obj=x_train.index, filepath_or_buffer=curr_saving_directory+'train_index.pkl')\n",
    "        pd.to_pickle(obj=x_test.index, filepath_or_buffer=curr_saving_directory+'test_index.pkl')\n",
    "        pd.to_pickle(obj=min_max_scaler, filepath_or_buffer=curr_saving_directory+'minmax_scaler.pkl')\n",
    "        with open('{}/training_cols.txt'.format(curr_saving_directory), 'w') as f:\n",
    "            f.write('\\n'.join(str(i) for i in training_cols))\n",
    "\n",
    "    classes_freq_training = y_train.value_counts()\n",
    "    use_balanced_class_weight = classes_freq_training.min() < classes_freq_training.max()*0.8\n",
    "    for estimator, grid_search_params in estimator_to_params_dict.items():\n",
    "        if already_trained[models_names_dict[str(estimator).split('(')[0]]]:\n",
    "          print(estimator, 'already trained')\n",
    "          continue\n",
    "        if not use_balanced_class_weight: ###only training with class_weight==None for balanced training set (i.e. undersample='min' or 'smote')\n",
    "            grid_search_params = [{param:param_values if param!='class_weight' else [None] for param,param_values in elem.items()} for elem in grid_search_params]\n",
    "        else:\n",
    "            grid_search_params = [{param:param_values if param!='class_weight' else ['balanced'] for param,param_values in elem.items()} for elem in grid_search_params]\n",
    "        print(estimator, grid_search_params)\n",
    "        whole_clf_gridsearch = GridSearchCV(estimator, n_jobs=3, cv=5, param_grid=grid_search_params, verbose=2)\n",
    "        whole_clf_gridsearch.fit(x_train.values, y_encoded)\n",
    "        predictions = whole_clf_gridsearch.predict(x_test.values)\n",
    "        print('Classification report:')\n",
    "        print(classification_report(y_test, [eval('inv_{}_label_mapping'.format(curr_dimension[0] if curr_dimension!='global' else curr_dimension))[pred] for pred in predictions]))\n",
    "        print('Confusion matrix:')\n",
    "        cm = confusion_matrix(eval('{}_label_encoder'.format(curr_dimension[0] if curr_dimension!='global' else curr_dimension)).transform(y_test), predictions)\n",
    "        print(cm)\n",
    "        labels = sorted(y_test.unique())\n",
    "        cm_df = pd.DataFrame(cm, index=labels, columns=labels)\n",
    "        cm_df.to_parquet(curr_saving_directory+'cm__best_{}.parquet'.format (models_names_dict[str(estimator).split('(')[0]]) )\n",
    "\n",
    "        pd.to_pickle(obj=whole_clf_gridsearch, filepath_or_buffer=curr_saving_directory+'grid_search_{}.pkl'.format(models_names_dict[str(estimator).split('(')[0]]))\n",
    "        tmp_estimators = get_best_params_and_scores_gridsearch(whole_clf_gridsearch)\n",
    "        for k,v in curr_params.items():\n",
    "            tmp_estimators[k]=v\n",
    "        tmp_estimators['estimator_type'] = str(estimator).split('(')[0]\n",
    "        tmp_estimators = tmp_estimators.reset_index().set_index(list(curr_params.keys())+['estimator_type','index'])\n",
    "        estimators = pd.concat([estimators, tmp_estimators],axis=0, ignore_index=False)\n",
    "\n",
    "    estimators.to_pickle(curr_model_directory+estimators_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BINARY MODELS\n",
    "\n",
    "#### Same approach as for multilabel models -> for each category, undersample=['min','smote', False], drop_undefined=[True, False] and so on... training a model for each parameters' combination and storing them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_model_directory='./data/models/ML/0/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'curr_dimension': 'horizontal', 'undersample': 'min', 'drop_undefined': True, 'drop_centrists': False}\n",
      "./data/models/ML/0/horizontal/bin/centrist/no_undefined/undersampled_min/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\onest\\AppData\\Local\\Temp\\ipykernel_10084\\2876654511.py:26: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dataset[curr_y_col+'_is_'+category_ovo] = tmp_s.copy()\n",
      "C:\\Users\\onest\\AppData\\Local\\Temp\\ipykernel_10084\\2876654511.py:26: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dataset[curr_y_col+'_is_'+category_ovo] = tmp_s.copy()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "horizontal_polcompass_flair_is_centrist\n",
      "1    63\n",
      "0    63\n",
      "Name: count, dtype: int64\n",
      "LinearSVC(max_iter=3000) [{'C': [0.1, 1, 10], 'class_weight': [None], 'loss': ['hinge', 'squared_hinge'], 'penalty': ['l2']}, {'C': [0.1, 1, 10], 'class_weight': [None], 'loss': ['squared_hinge'], 'penalty': ['l1']}]\n",
      "Fitting 5 folds for each of 9 candidates, totalling 45 fits\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    centrist       0.80      0.60      0.69        60\n",
      "non_centrist       0.20      0.40      0.27        15\n",
      "\n",
      "    accuracy                           0.56        75\n",
      "   macro avg       0.50      0.50      0.48        75\n",
      "weighted avg       0.68      0.56      0.60        75\n",
      "\n",
      "Confusion matrix:\n",
      "[[36 24]\n",
      " [ 9  6]]\n",
      "Classification report (TRAIN SET!):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    centrist       0.94      0.92      0.93        63\n",
      "non_centrist       0.92      0.94      0.93        63\n",
      "\n",
      "    accuracy                           0.93       126\n",
      "   macro avg       0.93      0.93      0.93       126\n",
      "weighted avg       0.93      0.93      0.93       126\n",
      "\n",
      "Confusion matrix (TRAIN SET!):\n",
      "[[58  5]\n",
      " [ 4 59]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from scipy.sparse import csr_matrix\n",
    "from scipy.sparse import hstack as scipy_hstack\n",
    "from scipy.sparse import vstack as scipy_vstack\n",
    "from pathlib import Path\n",
    "import time, os, pickle, itertools\n",
    "from utils import flatten\n",
    "\n",
    "min_max_scaler = MinMaxScaler()\n",
    "estimators_filename = 'estimators_bin.pkl'\n",
    "try:\n",
    "    estimators = pd.read_pickle(curr_model_directory+estimators_filename)\n",
    "except:\n",
    "    estimators = pd.DataFrame()\n",
    "\n",
    "models_params_dct = {'curr_dimension': ['horizontal'],\n",
    "                     'undersample':['min'],\n",
    "                     'drop_undefined':[True],\n",
    "                     'drop_centrists':[False],\n",
    "}\n",
    "#models_params_dct = {k:list(set(v)) for k,v in models_params_dct.items()}\n",
    "models_names_dict = {'LinearSVC': 'svc', 'RandomForestClassifier': 'rf', 'LogisticRegression': 'lr'}\n",
    "\n",
    "drop_nltk_entities = True\n",
    "populism_cols = ['Populism','PeopleCentrism','AntiElitism','EmotionalAppeal']\n",
    "curr_cols = pd.Index(set(features_cols+nrclex_cols+vice_cols+virtue_cols+bias_cols+intensity_cols+vad_cols+social_cols+detoxify_cols+populism_cols+ ['tokens_count','sentences_count'] + list(set(subcat_count_cols.values())) + list(subcat_count_cols.keys())) )\n",
    "if drop_nltk_entities:\n",
    "    curr_cols = curr_cols.drop(['nltk_organization_count', 'nltk_gsp_count', 'nltk_gpe_count', 'nltk_entities_count',\n",
    " 'nltk_location_count', 'nltk_facility_count', 'nltk_person_count'], errors='ignore')\n",
    "    \n",
    "for tup in itertools.product(*models_params_dct.values()):\n",
    "    curr_params = {list(models_params_dct.keys())[i]:tup[i] for i in range(len(tup))}\n",
    "    print(curr_params)\n",
    "    locals().update(curr_params)\n",
    "\n",
    "    curr_cat_col = category_col if curr_dimension=='global' else curr_dimension+'_'+category_col\n",
    "\n",
    "    train = pd.read_parquet(curr_model_directory+'train.parquet')\n",
    "    test = pd.read_parquet(curr_model_directory+'test.parquet')\n",
    "\n",
    "    train = train.loc[train[curr_cat_col]==train['{}Dimension'.format(curr_dimension.capitalize())]]\\\n",
    "                if curr_dimension in ['vertical','horizontal'] \\\n",
    "                else \\\n",
    "                    train.loc[(train['VerticalDimension'].fillna('centrist')==train['vertical_polcompass_flair'].fillna('centrist')) & (train['HorizontalDimension'].fillna('centrist')==train['horizontal_polcompass_flair'].fillna('centrist'))]\n",
    "    if drop_centrists:\n",
    "        train = train.loc[train[curr_cat_col]!='centrist']\n",
    "        if curr_dimension not in ['horizontal', 'vertical']:\n",
    "          train = train[(train['horizontal_polcompass_flair']!='centrist') & (train['vertical_polcompass_flair']!='centrist')]\n",
    "    if drop_undefined:\n",
    "        train = train.loc[train[curr_cat_col]!='Undefined']\n",
    "\n",
    "    test = test.loc[test['_'.join([curr_dimension, category_col])]==test['{}Dimension'.format(curr_dimension.capitalize())]] \\\n",
    "                if curr_dimension in ['vertical','horizontal'] \\\n",
    "                else \\\n",
    "                    test.loc[(test['VerticalDimension'].fillna('centrist')==test['vertical_polcompass_flair'].fillna('centrist')) & (test['HorizontalDimension'].fillna('centrist')==test['horizontal_polcompass_flair'].fillna('centrist'))]\n",
    "    if drop_centrists:\n",
    "        test = test.loc[test[curr_cat_col]!='centrist']\n",
    "        if curr_dimension not in ['horizontal', 'vertical']:\n",
    "          test = test[(test['horizontal_polcompass_flair']!='centrist') & (test['vertical_polcompass_flair']!='centrist')]\n",
    "    if drop_undefined:\n",
    "        test = test.loc[test[curr_cat_col]!='Undefined']\n",
    "\n",
    "\n",
    "    all_labels = train.loc[:, curr_cat_col].dropna().unique()\n",
    "    for curr_label in all_labels:\n",
    "        if curr_label.lower()=='undefined':\n",
    "          continue\n",
    "        tfidf_col = 'tfidf20_{}{}_{}'.format(curr_dimension, '_no_centrist_no_undefined' if drop_centrists and drop_undefined else '_no_centrist' if drop_centrists else '_no_undefined' if drop_undefined else '', curr_label)\n",
    "        curr_saving_directory = curr_model_directory+'{}/bin/{}/{}{}'.format(curr_dimension, curr_label, 'no_centrist_no_undefined/' if drop_centrists and drop_undefined else 'no_centrist/' if drop_centrists else 'no_undefined/' if drop_undefined else 'whole/', 'no_undersampling/' if not undersample else 'undersampled_max/' if undersample.lower()=='max' else 'smote/' if undersample.lower()=='smote' else 'undersampled_min/')\n",
    "        print(curr_saving_directory)\n",
    "        output_dir = Path(curr_saving_directory)\n",
    "        output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        already_trained = {model_name: 'cm_{}_best_precision.parquet'.format(model_name) in os.listdir(curr_saving_directory) for model_name in  models_names_dict.values()}\n",
    "        if os.path.exists(curr_saving_directory) and all(already_trained.values()):\n",
    "            continue\n",
    "\n",
    "        tfidf_train = pd.read_pickle(os.path.join(curr_model_directory, 'tfidf/train/{}.pkl'.format(tfidf_col))).rename(tfidf_col)\n",
    "        tfidf_train = pd.DataFrame(tfidf_train.dropna().tolist(), index=tfidf_train.dropna().index).rename(lambda x: 'word_feat_'+str(x), axis=1)\n",
    "        tfidf_test = pd.read_pickle(os.path.join(curr_model_directory, 'tfidf/test/{}.pkl'.format(tfidf_col))).rename(tfidf_col)\n",
    "        tfidf_test = pd.DataFrame(tfidf_test.dropna().tolist(), index=tfidf_test.dropna().index).rename(lambda x: 'word_feat_'+str(x), axis=1)\n",
    "        if undersample=='smote' and any(already_trained.values()):\n",
    "          curr_train = pd.read_parquet(curr_saving_directory+'train_smote.parquet')\n",
    "          if not any(curr_train.columns.str.contains('word_feat')):\n",
    "            print('wrong smote index... retraining')\n",
    "            already_trained={mod:False for mod in already_trained.keys()}\n",
    "        if any(already_trained.values()):\n",
    "          with open(curr_saving_directory+'training_cols.txt', 'r') as f:\n",
    "            training_cols = f.read().splitlines()\n",
    "          min_max_scaler=pd.read_pickle(curr_saving_directory+'minmax_scaler.pkl')\n",
    "\n",
    "          if undersample=='smote':\n",
    "            curr_train = pd.read_parquet(curr_saving_directory+'train_smote.parquet') #pass\n",
    "          else:\n",
    "            train_index = pd.read_pickle(curr_saving_directory+'train_index.pkl')\n",
    "            if sorted(train_index.join(train.index, how='inner')) != sorted(train_index):\n",
    "                raise ValueError('Wrong index for the current training set!!!')\n",
    "            curr_train = train.loc[train_index].join(tfidf_train, how='left').loc[:,training_cols+[curr_cat_col]]\n",
    "            curr_train[curr_cat_col]=(curr_train[curr_cat_col]==curr_label)\n",
    "          test_index = pd.read_pickle(curr_saving_directory+'test_index.pkl')\n",
    "          if sorted(test_index.join(test.index, how='inner')) != sorted(test_index):\n",
    "              raise ValueError('Wrong index for the current test set!!!')\n",
    "          curr_test = test.loc[test_index].join(tfidf_test, how='left').loc[:,training_cols+[curr_cat_col]]\n",
    "          curr_test[curr_cat_col]=(curr_test[curr_cat_col]==curr_label)\n",
    "        else:\n",
    "            if sorted(tfidf_train.index) != sorted(train.index):\n",
    "              raise ValueError('pd')\n",
    "            curr_train = train.join(tfidf_train, how='left')\n",
    "\n",
    "            curr_train = get_curr_dataset(curr_train, category_col=category_col, dimension=curr_dimension,\n",
    "                                          dropna=True, undersample=undersample, category_ovo=curr_label,\n",
    "                                      training_col=[c for c in curr_train.columns if c.startswith('word_feat_')]+list(curr_cols.drop([c for c in curr_train.columns if category_col in c or 'tfidf' in c], errors='ignore')))\n",
    "\n",
    "            if sorted(tfidf_test.index) != sorted(test.index):\n",
    "              raise ValueError('pd')\n",
    "            curr_test = test.join(tfidf_test, how='left')\n",
    "            curr_test = get_curr_dataset(curr_test, category_col=category_col, dimension=curr_dimension,\n",
    "                                          dropna=True, undersample=False, category_ovo=curr_label,\n",
    "                                      training_col=curr_train.columns[:-1] )\n",
    "        x_train = curr_train.iloc[:,:-1]\n",
    "        x_train.columns = x_train.columns.astype(str)\n",
    "        x_train = pd.DataFrame(min_max_scaler.fit_transform(x_train), columns=x_train.columns, index=x_train.index) #x_train.apply(lambda x: (x-x.min())/(x.max()-x.min()) if x.min()<0 or x.max()>1 else x) ###min-max normalization\n",
    "        y_train = curr_train.iloc[:,-1].map(lambda x: int(not x))   ##category col\n",
    "        print(y_train.value_counts())\n",
    "\n",
    "        training_cols = [c for c in x_train.columns if not c.isnumeric()] + [c for c in train.columns if 'tfidf' in c]\n",
    "        if undersample and undersample.lower()=='smote' and not any(already_trained.values()):\n",
    "            curr_train.to_parquet(curr_saving_directory+'train_smote.parquet')\n",
    "        del(curr_train, tfidf_train)\n",
    "\n",
    "        x_test = curr_test.loc[:, x_train.columns].rename(lambda x: str(x), axis=1).reindex(x_train.columns, axis=1)\n",
    "        x_test = pd.DataFrame(min_max_scaler.transform(x_test), columns=x_test.columns, index=x_test.index)\n",
    "        y_test = curr_test.iloc[:,-1].map(lambda x: int(not x)).map({0: 'non_'+curr_label, 1: curr_label})   ##category col\n",
    "        del(curr_test,tfidf_test,)\n",
    "\n",
    "        if not any(already_trained.values()):\n",
    "          with open('{}/training_cols.txt'.format(curr_saving_directory), 'w') as f:\n",
    "              f.write('\\n'.join(str(i) for i in training_cols))\n",
    "          pd.to_pickle(obj=x_train.index, filepath_or_buffer=curr_saving_directory+'train_index.pkl')\n",
    "          pd.to_pickle(obj=x_test.index, filepath_or_buffer=curr_saving_directory+'test_index.pkl')\n",
    "          pd.to_pickle(obj=min_max_scaler, filepath_or_buffer=curr_saving_directory+'minmax_scaler.pkl')\n",
    "\n",
    "        classes_freq_training = y_train.value_counts()\n",
    "        use_balanced_class_weight = classes_freq_training.min() < classes_freq_training.max()*0.8\n",
    "\n",
    "        for estimator, grid_search_params in estimator_to_params_dict.items():\n",
    "            if already_trained[models_names_dict[str(estimator).split('(')[0]]]:\n",
    "                print(estimator, 'already trained')\n",
    "                continue\n",
    "            if not use_balanced_class_weight: ###only training with class_weight==None for balanced training set (i.e. undersample='min' or 'smote')\n",
    "                grid_search_params = [{param:param_values if param!='class_weight' else [None] for param,param_values in elem.items()} for elem in grid_search_params]\n",
    "            else:\n",
    "                grid_search_params = [{param:param_values if param!='class_weight' else ['balanced'] for param,param_values in elem.items()} for elem in grid_search_params]\n",
    "            print(estimator, grid_search_params)\n",
    "            whole_clf_gridsearch = GridSearchCV(estimator, n_jobs=-4, cv=5, scoring='recall', param_grid=grid_search_params, verbose=2)\n",
    "            whole_clf_gridsearch.fit(x_train.values, y_train)\n",
    "            predictions = pd.Series(whole_clf_gridsearch.predict(x_test.values)).map({0: 'non_'+curr_label, 1: curr_label})\n",
    "            print('Classification report:')\n",
    "            print(classification_report(y_test, predictions))\n",
    "            print('Confusion matrix:')\n",
    "            cm = confusion_matrix(y_test, predictions)\n",
    "            print(cm)\n",
    "            labels = sorted( y_test.unique())\n",
    "            cm_df = pd.DataFrame(cm, index=labels, columns=labels)\n",
    "            cm_df.to_parquet(curr_saving_directory+'cm_{}_best_precision.parquet'.format(models_names_dict[str(estimator).split('(')[0]]))\n",
    "            \n",
    "            tmp_estimators = get_best_params_and_scores_gridsearch(whole_clf_gridsearch)\n",
    "            for k,v in curr_params.items():\n",
    "                tmp_estimators[k]=v\n",
    "            tmp_estimators['estimator_type'] = str(estimator).split('(')[0]\n",
    "            tmp_estimators['curr_label'] =  curr_label\n",
    "            tmp_estimators = tmp_estimators.reset_index().set_index(list(curr_params.keys())+['estimator_type','curr_label','index'])\n",
    "            estimators = pd.concat([estimators, tmp_estimators],axis=0, ignore_index=False)\n",
    "            pd.to_pickle(obj=whole_clf_gridsearch, filepath_or_buffer=curr_saving_directory+'grid_search_{}_precision.pkl'.format(models_names_dict[str(estimator).split('(')[0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### STORING PREDICTIONS ON TEST SET..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_dimension = 'horizontal'\n",
    "drop_undefined=False\n",
    "drop_centrists=False\n",
    "indexes = {}\n",
    "i=0\n",
    "\n",
    "dropped_cats_str = 'no_centrist_no_undefined' if drop_centrists and drop_undefined else 'no_undefined' if drop_undefined else 'no_centrist' if drop_centrists else 'whole'\n",
    "\n",
    "curr_test = pd.read_parquet(os.path.join(curr_model_directory, 'test.parquet'))\n",
    "for curr_label in ['left','right','centrist']:\n",
    "    for undersample in ['min', 'smote', False]:\n",
    "        curr_saving_directory = curr_model_directory+'{}/bin/{}/{}{}'.format(curr_dimension, curr_label, 'no_centrist_no_undefined/' if drop_centrists and drop_undefined else 'no_centrist/' if drop_centrists else 'no_undefined/' if drop_undefined else 'whole/', 'no_undersampling/' if not undersample else 'undersampled_max/' if undersample.lower()=='max' else 'smote/' if undersample.lower()=='smote' else 'undersampled_min/')\n",
    "\n",
    "        tfidf_col = 'tfidf20_{}{}_{}'.format(curr_dimension, '_no_centrist_no_undefined' if drop_centrists and drop_undefined else '_no_centrist' if drop_centrists else '_no_undefined' if drop_undefined else '', curr_label)\n",
    "\n",
    "        min_max_scaler = pd.read_pickle(curr_saving_directory+'minmax_scaler.pkl')\n",
    "        curr_test_index = pd.read_pickle(os.path.join(curr_saving_directory, 'test_index.pkl'))\n",
    "        \n",
    "        tfidf_test = pd.read_pickle(os.path.join(curr_model_directory, 'tfidf/test/{}.pkl'.format(tfidf_col))).rename(tfidf_col)\n",
    "        tfidf_test = pd.DataFrame(tfidf_test.dropna().tolist(), index=tfidf_test.dropna().index).rename(lambda x: 'word_feat_'+str(x), axis=1)\n",
    "        x_test = curr_test.join(tfidf_test, how='left').loc[curr_test_index,min_max_scaler.get_feature_names_out()].rename(lambda x: str(x), axis=1).reindex(min_max_scaler.get_feature_names_out(), axis=1)\n",
    "        x_test = pd.DataFrame(min_max_scaler.transform(x_test), columns=x_test.columns, index=x_test.index)\n",
    "        \n",
    "        for model in ['svc','rf','lr']:\n",
    "            grid_search = pd.read_pickle(os.path.join(curr_saving_directory, f'grid_search_{model}.pkl'))\n",
    "            curr_test.loc[curr_test_index, f'predicted_{curr_dimension}_{curr_label}_{dropped_cats_str}_{undersample}_{model}'] = grid_search.predict(x_test)\n",
    "\n",
    "curr_test.to_parquet(os.path.join(curr_model_directory,'test_pred.parquet'))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model='svc'\n",
    "undersample = 'min'\n",
    "curr_pred_cols = pd.Index([c for c in curr_test.columns if c=='HorizontalDimension' or ('predicted' in c.lower() and f'{undersample}_{model}' in c)])\n",
    "tuples_values = curr_test.loc[curr_test_index, curr_pred_cols].reset_index(drop=True).set_index('HorizontalDimension').value_counts().index\n",
    "filtered_dfs = [curr_test[(curr_test[curr_pred_cols.drop('HorizontalDimension')] == pd.Series(t, index=curr_pred_cols.drop('HorizontalDimension'))).all(axis=1)] for t in tuples_values]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "curr_model_directory = './data/models/ML/final/'\n",
    "dataset = pd.concat([pd.read_parquet(curr_model_directory+'train.parquet'),\n",
    "                    pd.read_parquet(curr_model_directory+'test.parquet')]\n",
    "                    ,axis=0, ignore_index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gemini_df = pd.read_parquet('./data/results_gemini.parquet')\n",
    "\n",
    "\n",
    "\n",
    "gemini_df.Populism = gemini_df.Populism.astype(float)\n",
    "gemini_df.AntiElitism = gemini_df.AntiElitism.astype(float)\n",
    "gemini_df.PeopleCentrism = gemini_df.PeopleCentrism.astype(float)\n",
    "gemini_df.EmotionalAppeal = gemini_df.EmotionalAppeal.astype(float)\n",
    "gemini_df.Libertarian = gemini_df.Libertarian.astype(float)\n",
    "gemini_df.Authoritarian = gemini_df.Authoritarian.astype(float)\n",
    "gemini_df.Left = gemini_df.Left.astype(float)\n",
    "gemini_df.Right = gemini_df.Right.astype(float)\n",
    "\n",
    "mapping_gemini = {'Right': 'right', 'Left': 'left', 'Centrist':'centrist', 'Libertarian':'lib', 'Authoritarian':'auth', 'Centrist':'centrist', 'None':None, 'Political':True, 'NonPolitical':False}\n",
    "def mapping_gemini_cat(curr_cat):\n",
    "    \"\"\"dimension = author_flair_mapping.__map_to_valid_dimension__(dimension)\n",
    "    if dimension is None:\n",
    "        raise ValueError('Dimension is not correct')\n",
    "    curr_mapping = h_mapping_gemini if dimension==author_flair_mapping.HORIZONTAL_DIMENSION else v_mapping_gemini if dimension==author_flair_mapping.VERTICAL_DIMENSION else {}\n",
    "    \"\"\"\n",
    "    return mapping_gemini.get(curr_cat, curr_cat)\n",
    "    \n",
    "    \n",
    "gemini_df['Political'] = gemini_df.Political.map(mapping_gemini_cat)#.fillna(df['HorizontalDimension'])\n",
    "gemini_df['HorizontalDimension'] = gemini_df.HorizontalDimension.map(mapping_gemini_cat)#.fillna(df['HorizontalDimension'])\n",
    "gemini_df['VerticalDimension'] = gemini_df.VerticalDimension.map(mapping_gemini_cat)\n",
    "\n",
    "author_labels = gemini_df.groupby('author').polcompass_flair.unique().map(lambda x: x[0] if len(x)==1 else x).rename('author_label')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gemini_df_joined = dataset[[\n",
    "    \"filtered_tokens\",\n",
    "    \"polcompass_flair\",\n",
    "    \"horizontal_polcompass_flair\",\n",
    "    \"vertical_polcompass_flair\",\n",
    "    \"Political\",\n",
    "    \"HorizontalDimension\",\n",
    "    \"VerticalDimension\"\n",
    "]].join(gemini_df.reset_index().set_index(dataset.index.names)[[\n",
    "    \n",
    "    \"polcompass_flair\",\n",
    "    \"Political\",\n",
    "    \"HorizontalDimension\",\n",
    "    \"VerticalDimension\",\n",
    "    \"Libertarian\",\n",
    "    \"Authoritarian\",\n",
    "    \"Left\",\n",
    "    \"Right\"\n",
    "]], how='outer', rsuffix='_gemini')\n",
    "\n",
    "gemini_df_joined.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "for dimension in ['horizontal','vertical']:\n",
    "    train_index = pd.read_pickle(os.path.join(curr_model_directory,dimension,'whole','no_undersampling', 'train_index.pkl'))\n",
    "    test_index = pd.read_pickle(os.path.join(curr_model_directory,dimension,'whole','no_undersampling', 'test_index.pkl'))\n",
    "    dataset_index = train_index.append(test_index).unique()\n",
    "    print(dataset.loc[dataset_index,f'{dimension.capitalize()}Dimension'].value_counts(dropna=False))\n",
    "    print(dataset.loc[dataset_index].groupby('author')[f'{dimension}_polcompass_flair'].agg(lambda x: x.dropna().unique()).map(lambda x: [flair for flair in x if flair is not None and flair.lower()!='undefined']).map(lambda x: x[0] if len(x)==1 else None if not len(x) else x).value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for curr_label in ['left','right','centrist']:\n",
    "    print(curr_label,'\\n',(cm:=pd.read_parquet(f'./data/models/ML/final/horizontal/bin/{curr_label}/whole/undersampled_min/cm_svc_best.parquet')), '\\n', classification_report_from_confusion_matrix(cm), '\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_test[curr_test[[c for c in curr_test.columns if 'predicted' in c.lower() and f'{undersample}_{model}' in c]].fillna(5).sum(axis=1).map(lambda x: x==0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "def classification_report_from_confusion_matrix(confusion_matrix, **args):\n",
    "    y_true = np.zeros(np.sum(confusion_matrix.values), dtype=int)\n",
    "    y_pred = np.copy(y_true)\n",
    "\n",
    "    i = 0\n",
    "    for target in range(len(confusion_matrix)):\n",
    "        for pred in range(len(confusion_matrix)):\n",
    "            n = confusion_matrix.values[target][pred]\n",
    "            y_true[i:i+n] = target\n",
    "            y_pred[i:i+n] = pred\n",
    "            i += n\n",
    "\n",
    "    return sklearn.metrics.classification_report(y_true, y_pred, target_names = confusion_matrix.index.tolist(), **args)\n",
    "    \n",
    "for mod in ['svc','rf','lr']:\n",
    "    cm1 = pd.read_parquet(f'./data/models/ML/final/horizontal/bin/centrist/no_undefined/undersampled_min/cm_{mod}_best_precision.parquet')\n",
    "    cm2 = pd.read_parquet(f'./data/models/ML/final/horizontal/bin/centrist/no_undefined/undersampled_min/cm_{mod}_best.parquet')\n",
    "    print(mod)\n",
    "    print(cm1)\n",
    "    print(classification_report_from_confusion_matrix(cm1))\n",
    "    print(cm2,'\\n', classification_report_from_confusion_matrix(cm2), '\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### CHECK MODELS NOT TRAINED YET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools,os\n",
    "\n",
    "curr_model_directory = './data/models/ML/final/'\n",
    "\n",
    "models_params_dct = {'curr_dimension': ['vertical','horizontal','global'],\n",
    "                         'undersample':['min',False, 'smote'],\n",
    "                         'drop_undefined':[False],\n",
    "                         'drop_centrists':[True,False],\n",
    "    }\n",
    "models_params_dct = {k:list(set(v)) for k,v in models_params_dct.items()}\n",
    "models_names_dict = {'LinearSVC': 'svc', 'RandomForestClassifier': 'rf', 'LogisticRegression': 'lr'}\n",
    "\n",
    "to_train={}\n",
    "for tup in itertools.product(*models_params_dct.values()):\n",
    "    curr_params = {list(models_params_dct.keys())[i]:tup[i] for i in range(len(tup))}\n",
    "    curr_dimension=curr_params['curr_dimension']\n",
    "    undersample=curr_params['undersample']\n",
    "    drop_undefined = curr_params['drop_undefined']\n",
    "    drop_centrists = curr_params['drop_centrists']\n",
    "    \n",
    "    curr_saving_directory = curr_model_directory+'{}/{}{}'.format(curr_dimension, 'no_centrist_no_undefined/' if drop_centrists and drop_undefined else 'no_centrist/' if drop_centrists else 'no_undefined/' if drop_undefined else 'whole/', 'no_undersampling/' if not undersample else 'undersampled_max/' if undersample.lower()=='max' else 'smote/' if undersample.lower()=='smote' else 'undersampled_min/')\n",
    "    if os.path.exists(curr_saving_directory):\n",
    "      already_trained = {model_name: 'cm__best_{}.parquet'.format(model_name) in os.listdir(curr_saving_directory) for model_name in  models_names_dict.values()}\n",
    "      if not all(already_trained.values()):\n",
    "        to_train[curr_saving_directory] = [k for k in already_trained if not already_trained[k]]\n",
    "    else:\n",
    "        to_train[curr_saving_directory] = list(models_names_dict.values())\n",
    "\n",
    "\n",
    "\n",
    "models_params_dct_bin = {'curr_dimension': ['vertical','horizontal','global'],\n",
    "                     'undersample':['min',False, 'smote'],\n",
    "                     'drop_undefined':[False,],\n",
    "                     'drop_centrists':[False],\n",
    "}\n",
    "train_labels = pd.read_parquet(curr_model_directory+'train.parquet', columns=['polcompass_flair','horizontal_polcompass_flair', 'vertical_polcompass_flair'])\n",
    "labels = {curr_dimension:train_labels[f'{curr_dimension}_polcompass_flair' if curr_dimension in ['horizontal', 'vertical'] else 'polcompass_flair'].dropna().unique() for curr_dimension in models_params_dct['curr_dimension']}\n",
    "to_train_bin = {}\n",
    "for tup in itertools.product(*models_params_dct_bin.values()):\n",
    "    curr_params = {list(models_params_dct_bin.keys())[i]:tup[i] for i in range(len(tup))}\n",
    "    curr_dimension=curr_params['curr_dimension']\n",
    "    undersample=curr_params['undersample']\n",
    "    drop_undefined = curr_params['drop_undefined']\n",
    "    drop_centrists = curr_params['drop_centrists']\n",
    "    curr_labels = labels[curr_dimension]\n",
    "    for curr_label in curr_labels:\n",
    "        curr_saving_directory = curr_model_directory+'{}/bin/{}/{}{}'.format(curr_dimension, curr_label, 'no_centrist_no_undefined/' if drop_centrists and drop_undefined else 'no_centrist/' if drop_centrists else 'no_undefined/' if drop_undefined else 'whole/', 'no_undersampling/' if not undersample else 'undersampled_max/' if undersample.lower()=='max' else 'smote/' if undersample.lower()=='smote' else 'undersampled_min/')\n",
    "        if os.path.exists(curr_saving_directory):\n",
    "            already_trained = {model_name: 'cm_{}_best.parquet'.format(model_name) in os.listdir(curr_saving_directory) for model_name in  models_names_dict.values()}\n",
    "            if not all(already_trained.values()):\n",
    "                to_train_bin[curr_saving_directory] = [k for k in already_trained if not already_trained[k]] \n",
    "        else:\n",
    "            to_train_bin[curr_saving_directory] = list(models_names_dict.values())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{k:v for (k,v) in to_train_bin.items() if not '/Undefined/' in k}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
